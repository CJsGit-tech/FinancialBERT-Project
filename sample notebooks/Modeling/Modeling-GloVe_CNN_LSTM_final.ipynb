{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6krgOo2jWTyN"
   },
   "source": [
    "## Install/ Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjzEo_3xGC-S"
   },
   "outputs": [],
   "source": [
    "# Download Language model\n",
    "! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lREdX258JFko"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "import scipy\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from transformers import AutoTokenizer,AutoModel,AutoConfig\n",
    "from summarizer import Summarizer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzW1GOv6PCH-",
    "tags": []
   },
   "source": [
    "## GloVe Embedding Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Tes8lnKPIG-"
   },
   "source": [
    "### check if GloVe embedding file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmbABJNUGIZu"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "path = Path(r'../data')\n",
    "glove_path = path / 'golve'\n",
    "\n",
    "if glove_path.is_dir():\n",
    "  print('file exists')\n",
    "elif not glove_path.is_dir():\n",
    "  glove_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "  with open(glove_path / 'golve_6B.zip', 'wb') as f:\n",
    "    request = requests.get(r'https://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    f.write(request.content)\n",
    "  \n",
    "  with zipfile.ZipFile(glove_path / 'golve_6B.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(glove_path)\n",
    "    print('finished downloading')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acaN0AE1PP4-"
   },
   "source": [
    "### load GloVe embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBpKFr7JI6pG"
   },
   "outputs": [],
   "source": [
    "vocab,embeddings = [],[]\n",
    "\n",
    "with open('../data/golve/glove.6B.100d.txt', 'r', encoding=\"utf-8\") as file:\n",
    "  for line in file:\n",
    "    values = line.split()\n",
    "    vocab.append(values[0])\n",
    "    vector = np.asarray(values[1:], \"float32\")\n",
    "    embeddings.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NA7WkpRrPVpU"
   },
   "source": [
    "### extract word & GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eer45iuQI-hk"
   },
   "outputs": [],
   "source": [
    "vocab_arr = np.array(vocab)\n",
    "embs_arr = np.array(embeddings)\n",
    "vocab_arr = np.insert(vocab_arr, 0, '<pad>')\n",
    "vocab_arr = np.insert(vocab_arr, 1, '<unk>')\n",
    "\n",
    "pad_emb_arr = np.zeros((1,embs_arr.shape[1]))   #embedding for '<pad>' token.\n",
    "unk_emb_arr = np.mean(embs_arr,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
    "\n",
    "embs_arr = np.vstack((pad_emb_arr,unk_emb_arr,embs_arr))\n",
    "vocab_dict = {word: idx for idx, word in enumerate(vocab_arr)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMZT3UBZPefZ"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZDd8ED0PjZH"
   },
   "source": [
    "### function to tokenize & clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLsK3BnxPn1M"
   },
   "outputs": [],
   "source": [
    "def tokenization_clean(text: 'str', nlp_model: 'spacy model'):\n",
    "  doc = nlp_model(text)\n",
    "  tok_aft_spacy = [re.sub(r'[^\\w\\s]', '', tok.lemma_.lower()) for tok in doc \n",
    "                   if not tok.is_stop\n",
    "                   and not tok.is_punct \n",
    "                   and not tok.like_num \n",
    "                   and not tok.like_url \n",
    "                   and not tok.is_space \n",
    "                   and not tok.like_email \n",
    "                   and not tok.is_left_punct \n",
    "                   and not tok.is_right_punct \n",
    "                   and not tok.is_digit \n",
    "                   and not tok.is_currency]\n",
    "  \n",
    "  join_tok_aft_spacy = ' '.join(tok_aft_spacy)\n",
    "  return join_tok_aft_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4sWCL1iPsfh"
   },
   "source": [
    "### function to load news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DrFWDGTPxRQ"
   },
   "outputs": [],
   "source": [
    "def load_dataset(directory: 'str', file_name: 'str', fn_tokenization_clean: 'function', \n",
    "                 nlp_model, training_data = True):\n",
    "  \n",
    "  tqdm.pandas()\n",
    "  path = Path(directory)\n",
    "  file_path = path / file_name\n",
    "  data = pd.read_csv(file_path)\n",
    "\n",
    "  data['tokenized_text'] = data['text'].progress_apply(lambda x: fn_tokenization_clean(x, nlp_model))\n",
    "\n",
    "  if training_data == True:\n",
    "    X_train = data.loc[(data['timestamp']<='2018-12-31'), ['timestamp', 'text', 'tokenized_text']].copy()\n",
    "    X_valid = data.loc[(data['timestamp']>='2019-01-01') & (data['timestamp']<='2019-12-31'), ['timestamp', 'text', 'tokenized_text']].copy()\n",
    "    y_train = data.loc[(data['timestamp']<='2018-12-31'), ['timestamp', 'topics', 'sentiment']].copy()\n",
    "    y_valid = data.loc[(data['timestamp']>='2019-01-01') & (data['timestamp']<='2019-12-31'), ['timestamp', 'topics', 'sentiment']].copy()\n",
    "\n",
    "    y_train = y_train.loc[:,['timestamp', 'sentiment']].copy()\n",
    "    y_train['sentiment'] = y_train['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "    y_valid = y_valid.loc[:,['timestamp', 'sentiment']].copy()\n",
    "    y_valid['sentiment'] = y_valid['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "  \n",
    "  elif training_data == False:\n",
    "    X = data.loc[:, ['timestamp', 'text', 'tokenized_text']].copy()\n",
    "    y = data.loc[:, ['timestamp', 'sentiment']].copy()\n",
    "    y['sentiment'] = y['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV0VNoJFhUcz"
   },
   "source": [
    "### function to load a fold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyHsIDzbhLWg"
   },
   "outputs": [],
   "source": [
    "def load_fold_dataset(directory: 'str', \n",
    "                      file_name: 'str', \n",
    "                      fn_tokenization_clean: 'function', \n",
    "                      nlp_model):\n",
    "  \n",
    "  tqdm.pandas()\n",
    "  path = Path(directory)\n",
    "  file_path = path / file_name\n",
    "  data = pd.read_csv(file_path)\n",
    "  data['tokenized_text'] = data['text'].progress_apply(lambda x: fn_tokenization_clean(x, nlp_model))\n",
    "  data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "  y = data[['sentiment']].copy()\n",
    "  return data, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPavmge9hbmV"
   },
   "source": [
    "### function to prepare 10 folds of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XUGpOSUhS1a"
   },
   "outputs": [],
   "source": [
    "def prepare_folds_dataset(cv_path: 'str', multiple_label = True):\n",
    "  cv_path = Path(cv_path)\n",
    "  fold_dataset = {'fold-1':{'train':None, 'valid':None}, \n",
    "                  'fold-2':{'train':None, 'valid':None}, \n",
    "                  'fold-3':{'train':None, 'valid':None}, \n",
    "                  'fold-4':{'train':None, 'valid':None}, \n",
    "                  'fold-5':{'train':None, 'valid':None}, \n",
    "                  'fold-6':{'train':None, 'valid':None}, \n",
    "                  'fold-7':{'train':None, 'valid':None}, \n",
    "                  'fold-8':{'train':None, 'valid':None}, \n",
    "                  'fold-9':{'train':None, 'valid':None}, \n",
    "                  'fold-10':{'train':None, 'valid':None}}\n",
    "\n",
    "  for i in cv_path.iterdir():\n",
    "    i.name\n",
    "    for j in i.iterdir():\n",
    "      if 'train' in j.name:\n",
    "        train_path = j\n",
    "\n",
    "        X_fold_train, y_fold_train = load_fold_dataset(directory = i, \n",
    "                                            file_name = train_path.name,\n",
    "                                            fn_tokenization_clean = tokenization_clean,\n",
    "                                            nlp_model = nlp_model, \n",
    "                                            multiple_label = multiple_label)\n",
    "        \n",
    "        fold_dataset[i.name]['train'] = (X_fold_train, y_fold_train)\n",
    "      elif 'valid' in j.name:\n",
    "        valid_path = j\n",
    "\n",
    "        X_fold_valid, y_fold_valid = load_fold_dataset(directory = i,\n",
    "                                            file_name = valid_path.name,\n",
    "                                            fn_tokenization_clean = tokenization_clean,\n",
    "                                            nlp_model = nlp_model, \n",
    "                                            multiple_label = multiple_label)\n",
    "        \n",
    "        fold_dataset[i.name]['valid'] = (X_fold_valid, y_fold_valid)\n",
    "  \n",
    "  return fold_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koyUOezuP8KF"
   },
   "source": [
    "### function to combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UC3NXSk8QEbk"
   },
   "outputs": [],
   "source": [
    "def combine_onehotencoder(data_original, data_encoder):\n",
    "  df_complete = pd.concat([data_original, data_encoder], axis = 1)\n",
    "  df_complete.drop(['text', 'sentiment', 'topics'], axis = 1, inplace = True)\n",
    "\n",
    "  return df_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFgNV3rlQG2I"
   },
   "source": [
    "### mapping word to index of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TRVcGzBQL1N"
   },
   "outputs": [],
   "source": [
    "def word2idx(data: 'DataFrame', target: 'DataFrame', max_length):\n",
    "  tok_word_id_list = []\n",
    "  for tok in data['tokenized_text']:\n",
    "    tok_word_id = [vocab_dict[i] if i in vocab_dict else vocab_dict['<unk>'] for i in tok.split(' ')]\n",
    "\n",
    "    if len(tok_word_id) < max_length:\n",
    "      tok_word_id.extend([vocab_dict['<pad>']]*(max_length - len(tok_word_id)))\n",
    "      tok_word_id_list.append(tok_word_id) #train_word to idx\n",
    "    \n",
    "    elif len(tok_word_id) >= max_length:\n",
    "      tok_word_id = tok_word_id[:max_length]\n",
    "      tok_word_id_list.append(tok_word_id)\n",
    "  \n",
    "  labels = target['sentiment'].values\n",
    "  return tok_word_id_list, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7bQy3m7QPxe"
   },
   "source": [
    "### datasets & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3j9X-jsJZLT"
   },
   "outputs": [],
   "source": [
    "class Word_Embedding_Dataset():\n",
    "    def __init__(self,data,targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x, y = self.data[idx].copy(), self.targets[idx].copy()\n",
    "        return x, y\n",
    "\n",
    "#############################################################\n",
    "def emb_make_loader(X, Y, shuffle=False,batch_size=400, drop = True):\n",
    "    \n",
    "    X_val = np.expand_dims(X, -1)\n",
    "    dataset = Word_Embedding_Dataset(data=X_val, targets=Y)\n",
    "    ################## SHUFFLE #############################\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle = shuffle, drop_last = drop)\n",
    "    #############################################################\n",
    "    return dataloader\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfhKib5kQwJr"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cShamdq0Jjxb"
   },
   "outputs": [],
   "source": [
    "nlp_model = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sZBR4lMiMDg"
   },
   "source": [
    "### prepare X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JA8zwpU-JlOO"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = load_dataset(directory = '../data/GloVe Model/News Article Text File', \n",
    "                                                  file_name = 'articles_2015_2019.csv', \n",
    "                                                  fn_tokenization_clean = tokenization_clean, \n",
    "                                                  nlp_model = nlp_model,\n",
    "                                                  training_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEyH9F9VJ9YR"
   },
   "outputs": [],
   "source": [
    "X_train_idx, y_train_1 = word2idx(data = X_train, target = y_train, max_length = 200)\n",
    "X_valid_idx, y_valid_1 = word2idx(data = X_valid, target = y_valid, max_length = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQev6IJpXUDe"
   },
   "outputs": [],
   "source": [
    "dataloader_all_train = emb_make_loader(X_train_idx, y_train_1, shuffle = False, batch_size=200, drop = True)\n",
    "dataloader_all_valid = emb_make_loader(X_valid_idx, y_valid_1, shuffle = False, batch_size=200, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpN_qrZ2iRIB"
   },
   "source": [
    "### prepare 10 folds 0f dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v0I7mJCiUMZ"
   },
   "outputs": [],
   "source": [
    "folds_data_path = Path(r'../data/GloVe Model/Intermediate Output/dict_folds_data_text_only.pickle')\n",
    "\n",
    "if folds_data_path.is_file():\n",
    "  with open(folds_data_path, 'rb') as f_1:\n",
    "    dict_folds_data = pickle.load(f_1)\n",
    "\n",
    "else:\n",
    "  dict_folds_data = prepare_folds_dataset(cv_path = r'../data/GloVe Model/Cross Validation_fold_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXiOUJrNYDbP"
   },
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iF1dopKX_Rs"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier_2(nn.Module):\n",
    "  def __init__(self, emb_size, emb_dim, batch, emb_arr, input_dim, hidden_dim, n_layer, dropout, output, device):\n",
    "    super(LSTMClassifier_2, self).__init__()\n",
    "    self.hidden_size = hidden_dim\n",
    "    self.num_layers = n_layer\n",
    "    self.batch_size = batch\n",
    "    self.device = device\n",
    "\n",
    "    emb_arr = torch.tensor(emb_arr).clone().detach()\n",
    "    emb_arr = emb_arr.type(torch.float)\n",
    "    emb_arr = emb_arr.to(device)\n",
    "    init_glove_emb = nn.Embedding(emb_size, emb_dim, padding_idx = 0, device = device)\n",
    "    init_glove_emb = init_glove_emb.from_pretrained(emb_arr)\n",
    "\n",
    "    self.word_embedding = init_glove_emb\n",
    "\n",
    "    self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                            out_channels = 1, \n",
    "                            kernel_size = (1, 100), \n",
    "                            stride = 1, \n",
    "                            device = device)\n",
    "    self.pool_1 = nn.MaxPool2d(kernel_size = (3, 1))\n",
    "    self.tanh_1 = nn.Tanh()\n",
    "\n",
    "    self.conv_2 = nn.Conv2d(in_channels = 1, \n",
    "                            out_channels = 1, \n",
    "                            kernel_size = (2, 100), \n",
    "                            stride = 1,\n",
    "                            device = device)\n",
    "    self.pool_2 = nn.MaxPool2d(kernel_size = (3, 1))\n",
    "    self.tanh_2 = nn.Tanh()\n",
    "\n",
    "    self.conv_3 = nn.Conv2d(in_channels = 1, \n",
    "                            out_channels = 1, \n",
    "                            kernel_size = (3, 100), \n",
    "                            stride = 1,\n",
    "                            device = device)\n",
    "    self.pool_3 = nn.MaxPool2d(kernel_size = (3, 1))\n",
    "    self.tanh_3 = nn.Tanh()\n",
    "\n",
    "    self.conv_4 = nn.Conv2d(in_channels = 1, \n",
    "                            out_channels = 1, \n",
    "                            kernel_size = (4, 100), \n",
    "                            stride = 1,\n",
    "                            device = device)\n",
    "    self.pool_4 = nn.MaxPool2d(kernel_size = (3, 1))\n",
    "    self.tanh_4 = nn.Tanh()\n",
    "\n",
    "    self.lstm = nn.LSTM(input_dim, hidden_dim, n_layer, dropout = dropout, batch_first = True, bidirectional = True).to(device)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.tanh_5 = nn.Tanh()\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc1 = nn.Linear(263*hidden_dim*2, output)\n",
    "    self.h0, self.c0 = self.init_hidden()\n",
    "\n",
    "  def init_hidden(self):\n",
    "    h0 = torch.zeros(self.num_layers*2, self.batch_size, self.hidden_size, dtype= torch.float, requires_grad=True).to(self.device)\n",
    "    c0 = torch.zeros(self.num_layers*2, self.batch_size, self.hidden_size, dtype= torch.float, requires_grad=True).to(self.device)\n",
    "\n",
    "    return h0, c0\n",
    "  \n",
    "  def forward(self, x):\n",
    "    \n",
    "    x = x.type(torch.int)\n",
    "    x = torch.swapaxes(x, 1, 2)\n",
    "    x = x.to(self.device)\n",
    "    \n",
    "\n",
    "    x_emb = self.word_embedding(x)\n",
    "    x_emb = x_emb.type(torch.float)\n",
    "    x_emb = x_emb.to(self.device)\n",
    "\n",
    "    out_1 = self.conv_1(x_emb)\n",
    "    out_1 = self.pool_1(out_1)\n",
    "    out_1 = self.tanh_1(out_1)\n",
    "\n",
    "    out_2 = self.conv_2(x_emb)\n",
    "    out_2 = self.pool_2(out_2)\n",
    "    out_2 = self.tanh_2(out_2)\n",
    "\n",
    "    out_3 = self.conv_3(x_emb)\n",
    "    out_3 = self.pool_3(out_3)\n",
    "    out_3 = self.tanh_3(out_3)\n",
    "\n",
    "    out_4 = self.conv_4(x_emb)\n",
    "    out_4 = self.pool_4(out_4)\n",
    "    out_4 = self.tanh_4(out_4)\n",
    "\n",
    "    out = torch.concat([out_1, out_2, out_3, out_4], dim = 2)\n",
    "\n",
    "    out = out.squeeze().unsqueeze(-1)\n",
    "    output, (hn, cn) = self.lstm(out, (self.h0.detach(), self.c0.detach()))\n",
    "    output = self.dropout(output)\n",
    "    output = self.tanh_5(output)\n",
    "    output = self.flatten(output)\n",
    "    output1 = self.fc1(output)\n",
    "    return output1\n",
    "\n",
    "def logit_to_prob(logit):\n",
    "  logit = logit.type(torch.float)\n",
    "  logit = logit.squeeze()\n",
    "  prob = torch.sigmoid(logit)\n",
    "\n",
    "  return prob\n",
    "\n",
    "def prob_to_label(prob):\n",
    "  prob = prob.squeeze()\n",
    "  label = torch.round(prob)\n",
    "\n",
    "  return label\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "def metric_accuracy_f1(y_pred_label_tuple, y, device):\n",
    "    acc = accuracy_score(y.detach().cpu(), y_pred_label_tuple.detach().cpu())\n",
    "    f1 = f1_score(y.detach().cpu(), y_pred_label_tuple.detach().cpu(), average = 'macro')\n",
    "    return acc, f1\n",
    "\n",
    "def train_step(dataloader, model, logit_to_prob_fn, prob_to_label_fn, metric_fn, optimizer, device):\n",
    "  model = model.to(device)\n",
    "  model.train()\n",
    "  bceloss = nn.BCEWithLogitsLoss().to(device)\n",
    "  train_loss, train_accuracy, train_f1 = 0, 0, 0\n",
    "\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    y_pred_logit_tuple = model(X)\n",
    "    y_pred_prob_tuple = logit_to_prob(y_pred_logit_tuple).to(device)\n",
    "    y_pred_label_tuple = prob_to_label(y_pred_prob_tuple).to(device)\n",
    "    batch_loss = bceloss(y_pred_logit_tuple.squeeze(), y.type(torch.float))\n",
    "    train_loss += batch_loss\n",
    "\n",
    "    batch_train_accuracy, batch_f1 = metric_fn(y_pred_label_tuple, y, device)\n",
    "    train_accuracy += batch_train_accuracy\n",
    "    train_f1 += batch_f1\n",
    "\n",
    "    # zero gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backpropagation\n",
    "    batch_loss.backward()\n",
    "\n",
    "    # Optimize parameters\n",
    "    optimizer.step()\n",
    "  \n",
    "  train_loss /= len(dataloader)\n",
    "  train_accuracy /= len(dataloader)\n",
    "  train_f1 /= len(dataloader)\n",
    "\n",
    "  return {'train_loss': train_loss, 'train_accuracy': train_accuracy, 'train_f1': train_f1}\n",
    "\n",
    "\n",
    "def test_step(dataloader, model, logit_to_prob_fn, prob_to_label_fn, metric_fn, device):\n",
    "  model = model.to(device)\n",
    "  model.eval()\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    bceloss = nn.BCEWithLogitsLoss().to(device)\n",
    "    test_loss, test_accuracy, test_f1 = 0, 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "      X, y = X.to(device), y.to(device)\n",
    "      y_pred_logit_tuple = model(X)\n",
    "      y_pred_prob_tuple = logit_to_prob(y_pred_logit_tuple).to(device)\n",
    "      y_pred_label_tuple = prob_to_label(y_pred_prob_tuple).to(device)\n",
    "      batch_loss = bceloss(y_pred_logit_tuple.squeeze(), y.type(torch.float)).to(device)\n",
    "      test_loss += batch_loss\n",
    "\n",
    "      batch_test_accuracy, batch_test_f1 = metric_fn(y_pred_label_tuple, y, device)\n",
    "      test_accuracy += batch_test_accuracy\n",
    "      test_f1 += batch_test_f1\n",
    "    \n",
    "    test_loss /= len(dataloader)\n",
    "    test_accuracy /= len(dataloader)\n",
    "    test_f1 /= len(dataloader)\n",
    "  return {'test_loss': test_loss, 'test_accuracy': test_accuracy, 'test_f1': test_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MU5WHMBYmL1"
   },
   "outputs": [],
   "source": [
    "emb_size = len(vocab_dict)\n",
    "emb_dim = embs_arr.shape[1]\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = LSTMClassifier_2(emb_size = emb_size, emb_dim = emb_dim, batch = 200, emb_arr = embs_arr, \n",
    "                       input_dim = 1, hidden_dim = 10, n_layer = 10, dropout = 0.2, output = 1, device = device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lf6ciXPLY4zc"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "epochs = 20\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.0006)\n",
    "complete_record = {'train_loss': [], 'valid_loss': [], \n",
    "                   'train_accuracy': [], 'valid_accuracy': [], \n",
    "                   'train_f1': [], 'valid_f1': []}\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print(f'Epoch: {epoch} -----------------')\n",
    "  train_output = train_step(dataloader = dataloader_all_train, \n",
    "                            model = model, \n",
    "                            logit_to_prob_fn = logit_to_prob, \n",
    "                            prob_to_label_fn = prob_to_label, \n",
    "                            metric_fn = metric_accuracy_f1,\n",
    "                            optimizer = optimizer, \n",
    "                            device = device)\n",
    "  \n",
    "  test_output = test_step(dataloader = dataloader_all_valid, \n",
    "                          model = model, \n",
    "                          logit_to_prob_fn = logit_to_prob, \n",
    "                          prob_to_label_fn = prob_to_label,\n",
    "                          metric_fn = metric_accuracy_f1,\n",
    "                          device = device)\n",
    "  complete_record['train_loss'].append(train_output['train_loss'].item())\n",
    "  complete_record['valid_loss'].append(test_output['test_loss'].item())\n",
    "  complete_record['train_accuracy'].append(train_output['train_accuracy'])\n",
    "  complete_record['valid_accuracy'].append(test_output['test_accuracy'])\n",
    "  complete_record['train_f1'].append(train_output['train_f1'])\n",
    "  complete_record['valid_f1'].append(test_output['test_f1'])\n",
    "\n",
    "  print(f'train_loss: {train_output[\"train_loss\"]: .5f}, test_lost: {test_output[\"test_loss\"]: .5f}, train_acc: {train_output[\"train_accuracy\"]: .2f}, test_acc: {test_output[\"test_accuracy\"]: .2f}, train_f1: {train_output[\"train_f1\"]: .5f}, test_f1: {test_output[\"test_f1\"]: .5f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGxD2udlyFp0"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(complete_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgroc8GQkB0y"
   },
   "source": [
    "### 10-fold Monte Carlo CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19wWLwMsgYnb"
   },
   "outputs": [],
   "source": [
    "folds = ['fold-1', 'fold-2', 'fold-3', 'fold-4', 'fold-5', 'fold-6', 'fold-7', 'fold-8', 'fold-9', 'fold-10']\n",
    "mccv_score_dict = {'fold-1': None, 'fold-2': None, 'fold-3': None, 'fold-4': None, 'fold-5': None,\n",
    "                   'fold-6': None, 'fold-7': None, 'fold-8': None, 'fold-9': None, 'fold-10': None}\n",
    "\n",
    "for fold in folds:\n",
    "  print(f'{fold}:')\n",
    "  X_train_fold, y_train_fold = dict_folds_data[fold]['train']\n",
    "  X_valid_fold, y_valid_fold = dict_folds_data[fold]['valid']\n",
    "  y_train_fold_encoded = y_train_fold['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "  y_valid_fold_encoded = y_valid_fold['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "  y_train_fold_encoded = pd.DataFrame(y_train_fold_encoded, columns = ['sentiment'])\n",
    "  y_valid_fold_encoded = pd.DataFrame(y_valid_fold_encoded, columns = ['sentiment'])\n",
    "\n",
    "\n",
    "  X_train_fold_idx, y_train_1 = word2idx(data = X_train_fold, target = y_train_fold_encoded, max_length = 200)\n",
    "  X_valid_fold_idx, y_valid_1 = word2idx(data = X_valid_fold, target = y_valid_fold_encoded, max_length = 200)\n",
    "  dataloader_fold_train = emb_make_loader(X_train_fold_idx, y_train_1, shuffle = False, batch_size=200, drop = True)\n",
    "  dataloader_fold_valid = emb_make_loader(X_valid_fold_idx, y_valid_1, shuffle = False, batch_size=200, drop = True)\n",
    "\n",
    "  emb_size = len(vocab_dict)\n",
    "  emb_dim = embs_arr.shape[1]\n",
    "  device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "  model = LSTMClassifier_2(emb_size = emb_size, emb_dim = emb_dim, batch = 200, emb_arr = embs_arr, \n",
    "                        input_dim = 1, hidden_dim = 10, n_layer = 10, dropout = 0.2, output = 1, device = device).to(device)\n",
    "\n",
    "  epochs = 20\n",
    "  torch.manual_seed(42)\n",
    "  device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "  optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.0006)\n",
    "  complete_record = {'train_loss': [], 'valid_loss': [], \n",
    "                    'train_accuracy': [], 'valid_accuracy': [], \n",
    "                    'train_f1': [], 'valid_f1': []}\n",
    "\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    train_output = train_step(dataloader = dataloader_all_train, \n",
    "                              model = model, \n",
    "                              logit_to_prob_fn = logit_to_prob, \n",
    "                              prob_to_label_fn = prob_to_label, \n",
    "                              metric_fn = metric_accuracy_f1,\n",
    "                              optimizer = optimizer, \n",
    "                              device = device)\n",
    "    \n",
    "    test_output = test_step(dataloader = dataloader_all_valid, \n",
    "                            model = model, \n",
    "                            logit_to_prob_fn = logit_to_prob, \n",
    "                            prob_to_label_fn = prob_to_label,\n",
    "                            metric_fn = metric_accuracy_f1,\n",
    "                            device = device)\n",
    "    complete_record['train_loss'].append(train_output['train_loss'].item())\n",
    "    complete_record['valid_loss'].append(test_output['test_loss'].item())\n",
    "    complete_record['train_accuracy'].append(train_output['train_accuracy'])\n",
    "    complete_record['valid_accuracy'].append(test_output['test_accuracy'])\n",
    "    complete_record['train_f1'].append(train_output['train_f1'])\n",
    "    complete_record['valid_f1'].append(test_output['test_f1'])\n",
    "  \n",
    "  mccv_score_dict[fold] = complete_record\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_8s6GbKaTis"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.precision', 10)\n",
    "\n",
    "folds = ['fold-1', 'fold-2', 'fold-3', 'fold-4', 'fold-5', 'fold-6', 'fold-7', 'fold-8', 'fold-9', 'fold-10']\n",
    "\n",
    "df_extracted = pd.DataFrame({'train_loss': [], 'valid_loss': [], \n",
    "                             'train_accuracy': [], 'valid_accuracy': [], \n",
    "                             'train_f1': [], 'valid_f1': []})\n",
    "\n",
    "for fold in folds:\n",
    "  df = pd.DataFrame(mccv_score_dict[fold])\n",
    "  df_extracted.loc[len(df_extracted)] = df.iloc[-1,:].to_dict().values()\n",
    "\n",
    "df_extracted['folds'] = folds\n",
    "df_extracted.loc[len(df_extracted)] = [df_extracted['train_loss'].mean(), df_extracted['valid_loss'].mean(), \n",
    "                                       df_extracted['train_accuracy'].mean(), df_extracted['valid_accuracy'].mean(), \n",
    "                                       df_extracted['train_f1'].mean(), df_extracted['valid_f1'].mean(), 'average']\n",
    "\n",
    "df_extracted.loc[len(df_extracted)] = [df_extracted.iloc[:-1,0].std(), df_extracted.iloc[:-1,1].std(), \n",
    "                                       df_extracted.iloc[:-1,2].std(), df_extracted.iloc[:-1,3].std(), \n",
    "                                       df_extracted.iloc[:-1,4].std(), df_extracted.iloc[:-1,5].std(), 'std']\n",
    "\n",
    "changed_column = ['folds'] + list(df_extracted.columns[:-1])\n",
    "df_extracted[changed_column]                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N18YjAL8Wh8y"
   },
   "source": [
    "### confirm the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUxadtWof4Uk"
   },
   "source": [
    "#### load/ prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7J6ZWArfjwG"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = load_dataset(directory = '../data/GloVe Model/News Article Text File', \n",
    "                                                  file_name = 'articles_2015_2019.csv', \n",
    "                                                  fn_tokenization_clean = tokenization_clean, \n",
    "                                                  nlp_model = nlp_model,\n",
    "                                                  training_data = True)\n",
    "\n",
    "X_train_idx, y_train_1 = word2idx(data = X_train, target = y_train, max_length = 200)\n",
    "X_valid_idx, y_valid_1 = word2idx(data = X_valid, target = y_valid, max_length = 200)\n",
    "\n",
    "dataloader_all_train = emb_make_loader(X_train_idx, y_train_1, shuffle = False, batch_size=200, drop = True)\n",
    "dataloader_all_valid = emb_make_loader(X_valid_idx, y_valid_1, shuffle = False, batch_size=200, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AV1N1iOZWkND"
   },
   "outputs": [],
   "source": [
    "emb_size = len(vocab_dict)\n",
    "emb_dim = embs_arr.shape[1]\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model_final = LSTMClassifier_2(emb_size = emb_size, emb_dim = emb_dim, batch = 200, emb_arr = embs_arr,\n",
    "                               input_dim = 1, hidden_dim = 10, n_layer = 10, dropout = 0.2, output = 1, device = device).to(device)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "epochs = 20\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "optimizer = torch.optim.Adam(params = model_final.parameters(), lr = 0.0006)\n",
    "complete_record_final = {'train_loss': [], 'valid_loss': [],\n",
    "                         'train_accuracy': [], 'valid_accuracy': [],\n",
    "                         'train_f1': [], 'valid_f1': []}\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "  print(f'Epoch: {epoch} -----------------')\n",
    "  train_output = train_step(dataloader = dataloader_all_train, \n",
    "                            model = model_final, \n",
    "                            logit_to_prob_fn = logit_to_prob, \n",
    "                            prob_to_label_fn = prob_to_label, \n",
    "                            metric_fn = metric_accuracy_f1,\n",
    "                            optimizer = optimizer, \n",
    "                            device = device)\n",
    "  \n",
    "  test_output = test_step(dataloader = dataloader_all_valid, \n",
    "                          model = model_final, \n",
    "                          logit_to_prob_fn = logit_to_prob, \n",
    "                          prob_to_label_fn = prob_to_label,\n",
    "                          metric_fn = metric_accuracy_f1,\n",
    "                          device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "692GpZmSf-AV"
   },
   "source": [
    "### performance evaluation & confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOE-rxzWhA33"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "np.random.seed(42)\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "  pred_y_batch_list = []\n",
    "  true_y_batch_list = []\n",
    "\n",
    "  for X, y in dataloader_all_valid:\n",
    "    logit_validate = model(X)\n",
    "    probability_validate = logit_to_prob(logit_validate)\n",
    "    label_validation = prob_to_label(probability_validate)\n",
    "\n",
    "    pred_y_batch_list.append(label_validation)\n",
    "    true_y_batch_list.append(y)\n",
    "  \n",
    "  pred_y_batch_all = torch.cat(pred_y_batch_list, dim = 0)\n",
    "  true_y_batch_all = torch.cat(true_y_batch_list, dim = 0)\n",
    "\n",
    "  confusion_matrix_result = confusion_matrix(true_y_batch_all, pred_y_batch_all.to('cpu'), normalize = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Unzyodt3ths8"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.2)\n",
    "plt.figure(figsize = (4, 4))\n",
    "confusion_matrix_result_heatmap = sns.heatmap(confusion_matrix_result, \n",
    "                                              cmap=\"Blues\", \n",
    "                                              annot = True, \n",
    "                                              fmt=\".2f\", annot_kws={'size': 15}, \n",
    "                                              xticklabels=['Negative', 'Positive'], \n",
    "                                              yticklabels=['Negative', 'Positive'])\n",
    "\n",
    "confusion_matrix_result_heatmap.set(xlabel='Predicted Label', ylabel='True Label', title = 'Sentiment')\n",
    "acc_score = accuracy_score(true_y_batch_all, pred_y_batch_all.to('cpu'))\n",
    "f1_score_macro = f1_score(true_y_batch_all, pred_y_batch_all.to('cpu'), average = 'macro')\n",
    "plt.show()\n",
    "print(f'\\nvalid accuracy score: {acc_score}, valid f1 score: {f1_score_macro}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6krgOo2jWTyN",
    "fMZT3UBZPefZ"
   ],
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1F9aBBknVzh3FF2cVF3UHgY7zOaP9rPW4",
     "timestamp": 1679679956038
    },
    {
     "file_id": "1pPYidA8yXAOfcNCGecoeFvNg4ldfUTjL",
     "timestamp": 1679400797047
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
