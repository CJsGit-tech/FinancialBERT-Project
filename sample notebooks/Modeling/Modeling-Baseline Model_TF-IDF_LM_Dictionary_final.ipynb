{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDHmsKfaPbiQ"
   },
   "source": [
    "## Install/ Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46rxgJm7ODch"
   },
   "outputs": [],
   "source": [
    "! pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwTRkhSwPgDA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import yfinance\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59addqYftrr6"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXwpJQsxVQh4"
   },
   "source": [
    "### load sp500 data & prepare technical index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdZs9IMzVQh5"
   },
   "outputs": [],
   "source": [
    "def load_sp500_data(start_date = '2014-12-31', end_date = '2021-01-05'):\n",
    "  sp = yfinance.Ticker('^GSPC')\n",
    "  sp_history = sp.history(start =start_date, end = end_date)\n",
    "  sp_history.reset_index(inplace = True)\n",
    "  sp_history['Date'] = pd.to_datetime(sp_history['Date'].dt.date)\n",
    "  sp_history.drop(labels = ['Dividends', 'Stock Splits'], axis = 1, inplace = True)\n",
    "  sp_history = sp_history.loc[:,['Date', 'Open', 'High', 'Low', 'Volume', 'Close']]\n",
    "  sp_history['Close+1day'] = sp_history['Close'].shift(-1)\n",
    "  sp_history.dropna(inplace = True)\n",
    "  sp_history['up_down'] = sp_history[['Close', 'Close+1day']].apply(lambda x: 1 if x['Close+1day'] > x['Close'] else 0, axis = 1)\n",
    "\n",
    "  appl = yfinance.Ticker('AAPL')\n",
    "  appl_history = appl.history(start =start_date, end = end_date)\n",
    "  appl_history.reset_index(inplace = True)\n",
    "  appl_history['Date'] = pd.to_datetime(appl_history['Date'].dt.date)\n",
    "  appl_history['Close'] = appl_history['Close'].pct_change()\n",
    "\n",
    "  msft = yfinance.Ticker('MSFT')\n",
    "  msft_history = msft.history(start =start_date, end = end_date)\n",
    "  msft_history.reset_index(inplace = True)\n",
    "  msft_history['Date'] = pd.to_datetime(msft_history['Date'].dt.date)\n",
    "  msft_history['Close'] = msft_history['Close'].pct_change()\n",
    "\n",
    "  amzn = yfinance.Ticker('AMZN')\n",
    "  amzn_history = amzn.history(start =start_date, end = end_date)\n",
    "  amzn_history.reset_index(inplace = True)\n",
    "  amzn_history['Date'] = pd.to_datetime(amzn_history['Date'].dt.date)\n",
    "  amzn_history['Close'] = amzn_history['Close'].pct_change()\n",
    "\n",
    "  nvda = yfinance.Ticker('NVDA')\n",
    "  nvda_history = nvda.history(start =start_date, end = end_date)\n",
    "  nvda_history.reset_index(inplace = True)\n",
    "  nvda_history['Date'] = pd.to_datetime(nvda_history['Date'].dt.date)\n",
    "  nvda_history['Close'] = nvda_history['Close'].pct_change()\n",
    "\n",
    "  brk = yfinance.Ticker('BRK-B')\n",
    "  brk_history = brk.history(start =start_date, end = end_date)\n",
    "  brk_history.reset_index(inplace = True)\n",
    "  brk_history['Date'] = pd.to_datetime(brk_history['Date'].dt.date)\n",
    "  brk_history['Close'] = brk_history['Close'].pct_change()\n",
    "\n",
    "  googl = yfinance.Ticker('GOOGL')\n",
    "  googl_history = googl.history(start =start_date, end = end_date)\n",
    "  googl_history.reset_index(inplace = True)\n",
    "  googl_history['Date'] = pd.to_datetime(googl_history['Date'].dt.date)\n",
    "  googl_history['Close'] = googl_history['Close'].pct_change()\n",
    "\n",
    "  tsla = yfinance.Ticker('TSLA')\n",
    "  tsla_history = tsla.history(start =start_date, end = end_date)\n",
    "  tsla_history.reset_index(inplace = True)\n",
    "  tsla_history['Date'] = pd.to_datetime(tsla_history['Date'].dt.date)\n",
    "  tsla_history['Close'] = tsla_history['Close'].pct_change()\n",
    "\n",
    "  meta = yfinance.Ticker('META')\n",
    "  meta_history = meta.history(start =start_date, end = end_date)\n",
    "  meta_history.reset_index(inplace = True)\n",
    "  meta_history['Date'] = pd.to_datetime(meta_history['Date'].dt.date)\n",
    "  meta_history['Close'] = meta_history['Close'].pct_change()\n",
    "\n",
    "  xom = yfinance.Ticker('XOM')\n",
    "  xom_history = xom.history(start =start_date, end = end_date)\n",
    "  xom_history.reset_index(inplace = True)\n",
    "  xom_history['Date'] = pd.to_datetime(xom_history['Date'].dt.date)\n",
    "  xom_history['Close'] = xom_history['Close'].pct_change()\n",
    "\n",
    "  jpm = yfinance.Ticker('JPM')\n",
    "  jpm_history = jpm.history(start =start_date, end = end_date)\n",
    "  jpm_history.reset_index(inplace = True)\n",
    "  jpm_history['Date'] = pd.to_datetime(jpm_history['Date'].dt.date)\n",
    "  jpm_history['Close'] = jpm_history['Close'].pct_change()\n",
    "\n",
    "  for i in [10, 20, 30]:\n",
    "    sp_history['MA'+str(i)] = sp_history['Close'].rolling(i).mean()\n",
    "\n",
    "  for df, name in zip([appl_history, msft_history, amzn_history, nvda_history, brk_history, \n",
    "                       googl_history, tsla_history, meta_history, xom_history, jpm_history], \n",
    "                      ['appl_Close', 'msft_Close', 'amzn_Close', 'nvda_Close', 'brk_Close', \n",
    "                       'googl_Close', 'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close']):\n",
    "    df = df[['Date', 'Close']].rename(columns = {'Close': name})\n",
    "    sp_history = sp_history.merge(df, left_on = 'Date', right_on = 'Date', how = 'left')\n",
    "  \n",
    "  sp_history = sp_history[(sp_history['Date'] >= '2014-12-31') & (sp_history['Date'] <= '2020-12-31')].copy()\n",
    "  sp_history.dropna(inplace = True)\n",
    "\n",
    "  return sp_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMUZcpjm83RU"
   },
   "source": [
    "### load text dataset & combine sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5nXLRtY8-CF"
   },
   "outputs": [],
   "source": [
    "def load_fold_dataset(directory: 'str', \n",
    "                      file_name: 'str',\n",
    "                      sp500_fn: 'function'):\n",
    "  \n",
    "  path = Path(directory)\n",
    "  file_path = path / file_name\n",
    "  data = pd.read_csv(file_path)\n",
    "  start_date = data['timestamp'].unique()[0]\n",
    "  end_date = data['timestamp'].unique()[-1]\n",
    "  sp500 = sp500_fn()\n",
    "\n",
    "  column_subset_X = ['Date','Close'] + list(sp500.columns[8:])\n",
    "\n",
    "  data_X = sp500.loc[(sp500['Date']>= start_date) & (sp500['Date'] <= end_date),column_subset_X].copy()\n",
    "  data_y = sp500.loc[(sp500['Date']>= start_date) & (sp500['Date'] <= end_date),['Date', 'up_down']]\n",
    "\n",
    "  return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7B3lCKlW629r"
   },
   "outputs": [],
   "source": [
    "def load_dataset(sp500_fn: 'function', \n",
    "                 training_data = True):\n",
    "\n",
    "  sp500 = sp500_fn()\n",
    "\n",
    "  column_subset_X = ['Date','Close'] + list(sp500.columns[8:])\n",
    "\n",
    "  data_X = sp500.loc[:,column_subset_X].copy()\n",
    "  data_y = sp500.loc[:,['Date', 'up_down']]\n",
    "  \n",
    "  if training_data == True:\n",
    "    X_train = data_X[data_X['Date']<='2018-12-31'].copy()\n",
    "    X_valid = data_X[(data_X['Date']>='2019-01-01') & (data_X['Date']<='2019-12-31')].copy()\n",
    "    y_train = data_y[data_y['Date']<='2018-12-31'].copy()\n",
    "    y_valid = data_y[(data_X['Date']>='2019-01-01') & (data_X['Date']<='2019-12-31')].copy()\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "  \n",
    "  elif training_data == False:\n",
    "    X_train = data_X[data_X['Date']<='2019-12-31'].copy()\n",
    "    X_test = data_X[(data_X['Date']>='2020-01-01')].copy()\n",
    "    y_train = data_y[data_y['Date']<='2019-12-31'].copy()\n",
    "    y_test = data_y[(data_X['Date']>='2020-01-01')].copy()\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdTo-yrj50_d"
   },
   "source": [
    "### function to prepare 10 folds data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCnfqzop57Vw"
   },
   "outputs": [],
   "source": [
    "def prepare_folds_dataset(cv_path: 'str'):\n",
    "  cv_path = Path(cv_path)\n",
    "  fold_dataset = {'fold-1':{'train':None, 'valid':None}, \n",
    "                  'fold-2':{'train':None, 'valid':None}, \n",
    "                  'fold-3':{'train':None, 'valid':None}, \n",
    "                  'fold-4':{'train':None, 'valid':None}, \n",
    "                  'fold-5':{'train':None, 'valid':None}, \n",
    "                  'fold-6':{'train':None, 'valid':None}, \n",
    "                  'fold-7':{'train':None, 'valid':None}, \n",
    "                  'fold-8':{'train':None, 'valid':None}, \n",
    "                  'fold-9':{'train':None, 'valid':None}, \n",
    "                  'fold-10':{'train':None, 'valid':None}}\n",
    "\n",
    "  for i in cv_path.iterdir():\n",
    "    i.name\n",
    "    for j in i.iterdir():\n",
    "      if 'train' in j.name:\n",
    "        train_path = j\n",
    "\n",
    "        X_fold_train, y_fold_train = load_fold_dataset(directory = i, \n",
    "                                            file_name = train_path.name,\n",
    "                                            sp500_fn = load_sp500_data)\n",
    "        fold_dataset[i.name]['train'] = (X_fold_train, y_fold_train)\n",
    "      elif 'valid' in j.name:\n",
    "        valid_path = j\n",
    "\n",
    "        X_fold_valid, y_fold_valid = load_fold_dataset(directory = i,\n",
    "                                            file_name = valid_path.name,\n",
    "                                            sp500_fn = load_sp500_data)\n",
    "        fold_dataset[i.name]['valid'] = (X_fold_valid, y_fold_valid)\n",
    "  \n",
    "  return fold_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_FqqPEU9PxM"
   },
   "source": [
    "### load loughran_mcdonal sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfQORc9JtrPo"
   },
   "outputs": [],
   "source": [
    "def loughran_mcdonald_dict(directory: 'str', file_name: 'str'):\n",
    "  path = Path(directory)\n",
    "  file_path = path / file_name\n",
    "  data = pd.read_csv(file_path)\n",
    "\n",
    "  data_part = data[['Word', 'Negative', 'Positive', \n",
    "                    'Uncertainty', 'Litigious', 'Strong_Modal', \n",
    "                    'Weak_Modal', 'Constraining']].copy()\n",
    "\n",
    "  data_part['Word'] = data_part['Word'].str.lower()\n",
    "\n",
    "  for i in list(data_part.columns)[1:]:\n",
    "    data_part[i] = data_part[i].apply(lambda x: 1 if x >0 else 0)\n",
    "  \n",
    "  data_part.drop(index = 50741, inplace = True) # drop nan values\n",
    "  data_part.reset_index(drop = True, inplace = True)\n",
    "\n",
    "  return data_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlZ3y8Hl9GgZ"
   },
   "source": [
    "### gridsearchcv, using 10 folds dataset (Linear SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVDXIfSD9dvS"
   },
   "outputs": [],
   "source": [
    "def gridsearchcv(folds_data: 'dict', parametergrid: 'list', model: 'function'):\n",
    "  folds = ['fold-1', 'fold-2', 'fold-3', 'fold-4', 'fold-5', 'fold-6', 'fold-7', 'fold-8', 'fold-9', 'fold-10']\n",
    "  score_dict = {'parameter': [], 'fold':[], 'train_accuracy':[], 'train_f1':[], 'valid_accuracy': [], 'valid_f1': []}\n",
    "\n",
    "  for parameter in tqdm(parametergrid):\n",
    "    model_initialized = model.set_params(classification__C = parameter['classification__C'], \n",
    "                                         classification__penalty = parameter['classification__penalty'])\n",
    "    \n",
    "    for fold in folds:\n",
    "      score_dict['parameter'].append(parameter)\n",
    "      X_train, y_train = folds_data[fold]['train']\n",
    "      X_valid, y_valid = folds_data[fold]['valid']\n",
    "      model_initialized.fit(X_train, y_train['up_down'].values)\n",
    "      train_acc = model_initialized.score(X_train, y_train['up_down'].values)\n",
    "      predicted_y_train = model_initialized.predict(X_train)\n",
    "      train_f1 = f1_score(y_train['up_down'].values, predicted_y_train, average = 'macro')\n",
    "      valid_acc = model_initialized.score(X_valid, y_valid['up_down'].values)\n",
    "      predicted_y_valid = model_initialized.predict(X_valid)\n",
    "      valid_f1 = f1_score(y_valid['up_down'].values, predicted_y_valid, average = 'macro')\n",
    "\n",
    "      score_dict['fold'].append(fold)\n",
    "      score_dict['train_accuracy'].append(train_acc)\n",
    "      score_dict['train_f1'].append(train_f1)\n",
    "      score_dict['valid_accuracy'].append(valid_acc)\n",
    "      score_dict['valid_f1'].append(valid_f1)\n",
    "  \n",
    "  return score_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdvT2iW_2uo8"
   },
   "source": [
    "### function to visualize gridsearchcv scores (linearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME7nYySu2tk1"
   },
   "outputs": [],
   "source": [
    "def visualize_gridsearchcv(cv_result: 'df'):\n",
    "  plt.figure(figsize = (12, 6))\n",
    "  plt.plot(cv_result['classification__C'], cv_result[('valid_f1', 'mean')], label = 'valid f1 macro', color = 'blue')\n",
    "  plt.errorbar(x = cv_result['classification__C'], y = cv_result[('valid_f1', 'mean')], yerr = cv_result[('valid_f1', 'std')], label = '+/- 1 std', color = 'blue')\n",
    "  \n",
    "  for row in cv_result.itertuples():\n",
    "    plt.annotate('{:.4f}'.format(row[-2]), xy = (row[1] + 0.0001, row[-2] + 0.0002), color = 'black', fontsize = 10)\n",
    "    plt.annotate(f'std: {row[-1]:.4f}', xy = (row[1] + 0.0001, row[-2] + 0.005), color = 'red', fontsize = 10)\n",
    "  \n",
    "  plt.legend()\n",
    "  plt.xlim((0, 0.055))\n",
    "  plt.xlabel('linear SVC, C value')\n",
    "  plt.ylabel('valid f1 score (macro)')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAG43EdAtAGQ"
   },
   "source": [
    "### gridsearchcv, using 10 folds dataset (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nt1auF4dtENq"
   },
   "outputs": [],
   "source": [
    "def gridsearchcv_xgb(folds_data: 'dict', parametergrid: 'list', model: 'function'):\n",
    "  folds = ['fold-1', 'fold-2', 'fold-3', 'fold-4', 'fold-5', 'fold-6', 'fold-7', 'fold-8', 'fold-9', 'fold-10']\n",
    "  score_dict = {'parameter': [], 'fold':[], 'train_accuracy':[], 'train_f1':[], 'valid_accuracy': [], 'valid_f1': []}\n",
    "\n",
    "  for parameter in tqdm(parametergrid):\n",
    "    model_initialized = model.set_params(classification__learning_rate = parameter['classification__learning_rate'], \n",
    "                                         classification__max_depth = parameter['classification__max_depth'])\n",
    "    \n",
    "    for fold in folds:\n",
    "      score_dict['parameter'].append(parameter)\n",
    "      X_train, y_train = folds_data[fold]['train']\n",
    "      X_valid, y_valid = folds_data[fold]['valid']\n",
    "      model_initialized.fit(X_train, y_train['up_down'].values)\n",
    "      train_acc = model_initialized.score(X_train, y_train['up_down'].values)\n",
    "      predicted_y_train = model_initialized.predict(X_train)\n",
    "      train_f1 = f1_score(y_train['up_down'].values, predicted_y_train, average = 'macro')\n",
    "      valid_acc = model_initialized.score(X_valid, y_valid['up_down'].values)\n",
    "      predicted_y_valid = model_initialized.predict(X_valid)\n",
    "      valid_f1 = f1_score(y_valid['up_down'].values, predicted_y_valid, average = 'macro')\n",
    "\n",
    "      score_dict['fold'].append(fold)\n",
    "      score_dict['train_accuracy'].append(train_acc)\n",
    "      score_dict['train_f1'].append(train_f1)\n",
    "      score_dict['valid_accuracy'].append(valid_acc)\n",
    "      score_dict['valid_f1'].append(valid_f1)\n",
    "  \n",
    "  return score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEQ1Xyt6c-89"
   },
   "source": [
    "### function to visualize gridsearchcv result (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ovbq8GIjdHg9"
   },
   "outputs": [],
   "source": [
    "def visualize_gridsearchcv_xgb(cv_result: 'df'):\n",
    "  depth_list = [3, 4, 5]  \n",
    "  df_cv_result = cv_result[['classification__learning_rate',\n",
    "                            'classification__max_depth',\n",
    "                            'valid_f1']].copy()\n",
    "                               \n",
    "  df_cv_result.rename(columns = {'classification__learning_rate': 'learning_rate', \n",
    "                                 'classification__max_depth': 'max_depth'}, \n",
    "                      inplace = True)\n",
    "\n",
    "  plt.figure(figsize = (8, 6))\n",
    "\n",
    "  for idx, depth_no in zip(range(len(depth_list)), depth_list):\n",
    "    ax = plt.subplot(3, 1, idx+1)\n",
    "    ax.plot(df_cv_result.loc[(df_cv_result['max_depth'] == depth_no), 'learning_rate'], \n",
    "            df_cv_result.loc[(df_cv_result['max_depth'] == depth_no), ('valid_f1', 'mean')].values,\n",
    "            color = 'blue', \n",
    "            linewidth = 1, \n",
    "            label = f'depth: {depth_no}')\n",
    "    ax.errorbar(x = df_cv_result.loc[(df_cv_result['max_depth'] == depth_no), 'learning_rate'],\n",
    "                y = df_cv_result.loc[(df_cv_result['max_depth'] == depth_no), ('valid_f1', 'mean')].values, \n",
    "                yerr = df_cv_result.loc[(df_cv_result['max_depth'] == depth_no), ('valid_f1', 'std')].values, \n",
    "                label = '+/- 1 std', color = 'blue')\n",
    "    \n",
    "    for row in df_cv_result.loc[(df_cv_result['max_depth'] == depth_no),:].itertuples():\n",
    "      ax.annotate(f'{row[-2]:.4f}', (row[1], row[-2] + 0.001), fontsize = 10)\n",
    "      ax.annotate(f'{row[-1]:.4f}', (row[1], row[-1] + 0.004), fontsize = 10)\n",
    "\n",
    "    \n",
    "    ax.set_ylim((0.3, 0.6))\n",
    "    ax.set_ylabel('mean f1', fontsize = 12)\n",
    "    plt.yticks(fontsize=9)\n",
    "    ax.legend(fontsize = 10)\n",
    "\n",
    "    if idx != len(depth_list) - 1:\n",
    "      ax.tick_params(labelbottom=False)\n",
    "    \n",
    "    if idx == 0:\n",
    "      plt.title('validation f1 macro of the grid search cv results')\n",
    "    \n",
    "  plt.tight_layout(pad = 0.5)\n",
    "  plt.xlabel('XGBoost, learning rate', fontsize = 12)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "899zwDcu-OLa"
   },
   "source": [
    "### function to visualize confusion matrix (binary label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8ShYoZu-N1s"
   },
   "outputs": [],
   "source": [
    "def performance_metrics_binary(model, data, true_y, train_valid_test: 'str'):\n",
    "  predicted_y = model.predict(data)\n",
    "  accuracy_score = model.score(data, true_y)\n",
    "  f1score = f1_score(true_y, predicted_y, average = 'macro')\n",
    "  \n",
    "\n",
    "  plt.figure(figsize = (4, 4))\n",
    "  sns.set(font_scale=1.2)\n",
    "  cm_result = confusion_matrix(true_y, predicted_y, normalize = 'pred')\n",
    "\n",
    "  confusion_matrix_result_heatmap = sns.heatmap(cm_result, \n",
    "                                                cmap=\"Blues\", \n",
    "                                                annot = True, \n",
    "                                                fmt=\".2f\", annot_kws={'size': 15}, \n",
    "                                                xticklabels=['Negative', 'Positive'], \n",
    "                                                yticklabels=['Negative', 'Positive'])\n",
    "\n",
    "  confusion_matrix_result_heatmap.set(xlabel='Predicted Label', ylabel='True Label', title = 'price movement')\n",
    "\n",
    "  plt.show()\n",
    "  print(f'\\n{train_valid_test} accuracy: {accuracy_score}, {train_valid_test} f1 score: {f1score}')\n",
    "  return accuracy_score, f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0_yDJy6R44J"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PSnxwQUASvn"
   },
   "source": [
    "### prepare loughran-mcdonald sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDnU5ckbAX5f"
   },
   "outputs": [],
   "source": [
    "lm_sent_dict = loughran_mcdonald_dict(directory = '../data/TF-IDF Models', \n",
    "                                      file_name = 'Loughran-McDonald_MasterDictionary_1993-2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD4njH2_72P9"
   },
   "source": [
    "### load 10 folds dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dT8nq2l-71PV"
   },
   "outputs": [],
   "source": [
    "folds_data_path = Path(r'../data/TF-IDF Models/Intermediate Output/dict_folds_data.pickle')\n",
    "\n",
    "if folds_data_path.is_file():\n",
    "  with open(folds_data_path, 'rb') as f_1:\n",
    "    dict_folds_data = pickle.load(f_1)\n",
    "\n",
    "else:\n",
    "  dict_folds_data = prepare_folds_dataset(cv_path = r'../data/TF-IDF Models/Cross Validation_fold_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCE1XeUImrpJ"
   },
   "source": [
    "### load and split training/ valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zmt5jF-qlaFW"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = load_dataset(sp500_fn = load_sp500_data, training_data = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTLfSx9JSZIw"
   },
   "source": [
    "## Build LinearSVC Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djmOu2a6BDWm"
   },
   "source": [
    "### initialize necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLugrZA572n1"
   },
   "outputs": [],
   "source": [
    "minmaxscaler_price = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij19E1mPVCiy"
   },
   "source": [
    "### build pipeline (binary label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05mtNKsqQXrc"
   },
   "outputs": [],
   "source": [
    "ma_pipeline = Pipeline(steps = [('norm_price', minmaxscaler_price)])\n",
    "\n",
    "column_transformer = ColumnTransformer(transformers = [('price_ma', ma_pipeline, ['Close', 'MA10', 'MA20', 'MA30',\n",
    "                                                                                  'appl_Close', 'msft_Close', 'amzn_Close', \n",
    "                                                                                  'nvda_Close', 'brk_Close', 'googl_Close', \n",
    "                                                                                  'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close'])])\n",
    "\n",
    "clf_pipeline_binary = Pipeline(steps = [('preprocessing', column_transformer), \n",
    "                                        ('classification',LinearSVC(C= 0.05, class_weight = 'balanced'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0jR6Qk53f39"
   },
   "source": [
    "### 10-fold gridsearchcv (Linear SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6z_ZxK0lnPkY"
   },
   "outputs": [],
   "source": [
    "parameters_grid = {'classification__C': [0.001, 0.005, 0.01, 0.05], \n",
    "                   'classification__penalty': ['l2']}\n",
    "\n",
    "parameters = ParameterGrid(parameters_grid)\n",
    "\n",
    "result = gridsearchcv(folds_data = dict_folds_data, parametergrid = parameters, model = clf_pipeline_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENQvpLwwnctz"
   },
   "outputs": [],
   "source": [
    "parameters_grid_1 = {'classification__C': [], 'classification__penalty': []}\n",
    "\n",
    "df_result = pd.DataFrame(result)\n",
    "\n",
    "for parameter in df_result['parameter']:\n",
    "  for key, value in parameter.items():\n",
    "    parameters_grid_1[key].append(value)\n",
    "\n",
    "df_result_1 = df_result.merge(pd.DataFrame(parameters_grid_1), how = 'left', left_index = True, right_index = True)\n",
    "df_result_complete = df_result_1.iloc[:,1:].groupby(['classification__C',\n",
    "                                                     'classification__penalty'], as_index = False).agg({'train_accuracy': ['mean', 'std'], 'train_f1': ['mean', 'std'], 'valid_accuracy': ['mean', 'std'], 'valid_f1': ['mean', 'std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lk19gObEqGWQ"
   },
   "outputs": [],
   "source": [
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6tvHInloChu"
   },
   "outputs": [],
   "source": [
    "df_result_complete.iloc[df_result_complete[('valid_f1', 'mean')].idxmax()] # the best combination of parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJpYGyeZq6bI"
   },
   "source": [
    "### visualize gridsearchcv result (Linear SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3m-RPmJPrBMd"
   },
   "outputs": [],
   "source": [
    "visualize_gridsearchcv(cv_result = df_result_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bPncZuWqWh9"
   },
   "source": [
    "### confirm the best model (Linear SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqog6itAoJeE"
   },
   "outputs": [],
   "source": [
    "minmaxscaler_price = MinMaxScaler()\n",
    "\n",
    "ma_pipeline = Pipeline(steps = [('norm_price', minmaxscaler_price)])\n",
    "\n",
    "column_transformer = ColumnTransformer(transformers = [('price_ma', ma_pipeline, ['Close', 'MA10', 'MA20', 'MA30',\n",
    "                                                                                  'appl_Close', 'msft_Close', 'amzn_Close', \n",
    "                                                                                  'nvda_Close', 'brk_Close', 'googl_Close', \n",
    "                                                                                  'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close'])])\n",
    "\n",
    "clf_pipeline_binary = Pipeline(steps = [('preprocessing', column_transformer), \n",
    "                                        ('classification',LinearSVC(C= 0.05, class_weight = 'balanced'))])\n",
    "\n",
    "clf_pipeline_binary.fit(X_train, y_train['up_down'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWTpa4aQ5G8G"
   },
   "source": [
    "### performance evaluation and confusion matrix (train dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNuZw9zSqzWu"
   },
   "outputs": [],
   "source": [
    "accuracy_score_train, f1score_train = performance_metrics_binary(model = clf_pipeline_binary, \n",
    "                                                                 data = X_train, \n",
    "                                                                 true_y = y_train['up_down'].values, \n",
    "                                                                 train_valid_test = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JzfvzTp5Mb4"
   },
   "source": [
    "### performance evaluation & confusion matrix (valid dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ee_Eg9u15_P"
   },
   "outputs": [],
   "source": [
    "accuracy_score_valid, f1score_valid = performance_metrics_binary(model = clf_pipeline_binary, \n",
    "                                                                 data = X_valid, \n",
    "                                                                 true_y = y_valid['up_down'].values, \n",
    "                                                                 train_valid_test = 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ymm3jxuk5Wsz"
   },
   "source": [
    "## Build XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h14zEZt0EUzN"
   },
   "source": [
    "### build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLi4Bp202VLU"
   },
   "outputs": [],
   "source": [
    "minmaxscaler_price_xgboost = MinMaxScaler()\n",
    "ma_pipeline_xg = Pipeline(steps = [('norm_price', minmaxscaler_price_xgboost)])\n",
    "column_transformer_xg = ColumnTransformer(transformers = [('price_ma', ma_pipeline_xg, ['Close', 'MA10', 'MA20', 'MA30',\n",
    "                                                                                        'appl_Close', 'msft_Close', 'amzn_Close', \n",
    "                                                                                        'nvda_Close', 'brk_Close', 'googl_Close', \n",
    "                                                                                        'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close'])])\n",
    "clf_xgb = XGBClassifier(booster = 'gbtree', min_split_loss = 0.01, learning_rate = 0.01, max_depth = 3, n_estimators = 1000, scale_pos_weight = 0.9)\n",
    "clf_pipeline_binary_xgboost = Pipeline(steps = [('preprocessing', column_transformer_xg), \n",
    "                                                ('classification',clf_xgb)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5Pogu1dEN3W"
   },
   "source": [
    "### 10-fold gridsearchcv (XGBoost Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5X50roM6Khk"
   },
   "outputs": [],
   "source": [
    "parameters_grid_xgb = {'classification__learning_rate': [0.0005, 0.0007, 0.001], \n",
    "                   'classification__max_depth': [3, 4 , 5]}\n",
    "\n",
    "parameters_xgb = ParameterGrid(parameters_grid_xgb)\n",
    "\n",
    "result_xgb = gridsearchcv_xgb(folds_data = dict_folds_data, parametergrid = parameters_xgb, model = clf_pipeline_binary_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BoDXSGM6PxW"
   },
   "outputs": [],
   "source": [
    "parameters_grid_xgb_1 = {'classification__learning_rate': [], 'classification__max_depth': []}\n",
    "\n",
    "df_result_xgb = pd.DataFrame(result_xgb)\n",
    "\n",
    "for parameter in df_result_xgb['parameter']:\n",
    "  for key, value in parameter.items():\n",
    "    parameters_grid_xgb_1[key].append(value)\n",
    "\n",
    "df_result_xgb_1 = df_result_xgb.merge(pd.DataFrame(parameters_grid_xgb_1), how = 'left', left_index = True, right_index = True)\n",
    "df_result_xgb_complete = df_result_xgb_1.iloc[:,1:].groupby(['classification__learning_rate',\n",
    "                                                             'classification__max_depth'], as_index = False).agg({'train_accuracy': ['mean', 'std'], 'train_f1': ['mean', 'std'], 'valid_accuracy': ['mean', 'std'], 'valid_f1': ['mean', 'std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PxhIH4TDVb6"
   },
   "outputs": [],
   "source": [
    "df_result_xgb_complete.iloc[df_result_xgb_complete[('valid_f1', 'mean')].idxmax()] # the best combination of parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNGyNncwFugE"
   },
   "outputs": [],
   "source": [
    "df_result_xgb_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2qZdlJhDMWL"
   },
   "outputs": [],
   "source": [
    "df_cv_result = df_result_xgb_complete[['classification__learning_rate',\n",
    "                            'classification__max_depth',\n",
    "                            'valid_f1']].copy()\n",
    "                               \n",
    "df_cv_result.rename(columns = {'classification__learning_rate': 'learning_rate', \n",
    "                               'classification__max_depth': 'max_depth'}, \n",
    "                      inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WajjOP4Ef6F"
   },
   "source": [
    "### visualize gridsearchcv result (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1xcLbgK92pM"
   },
   "outputs": [],
   "source": [
    "visualize_gridsearchcv_xgb(df_result_xgb_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvwxsb_PH5Ha"
   },
   "source": [
    "### confirm the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nag3OrCh99hk"
   },
   "outputs": [],
   "source": [
    "minmaxscaler_price_xgboost = MinMaxScaler()\n",
    "ma_pipeline_xg = Pipeline(steps = [('norm_price', minmaxscaler_price_xgboost)])\n",
    "column_transformer_xg = ColumnTransformer(transformers = [('price_ma', ma_pipeline_xg, ['Close', 'MA10', 'MA20', 'MA30',\n",
    "                                                                                        'appl_Close', 'msft_Close', 'amzn_Close', \n",
    "                                                                                        'nvda_Close', 'brk_Close', 'googl_Close', \n",
    "                                                                                        'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close'])])\n",
    "clf_xgb = XGBClassifier(booster = 'gbtree', min_split_loss = 0.01, learning_rate = 0.001, max_depth = 4, n_estimators = 1000, scale_pos_weight = 0.9)\n",
    "clf_pipeline_binary_xgboost = Pipeline(steps = [('preprocessing', column_transformer_xg), \n",
    "                                                ('classification',clf_xgb)])\n",
    "\n",
    "clf_pipeline_binary_xgboost.fit(X_train, y_train['up_down'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsVa4u7ZIW1D"
   },
   "source": [
    "### performance evaluation & confusion matrix (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oijVUNfTIRRt"
   },
   "outputs": [],
   "source": [
    "accuracy_score_valid, f1score_valid = performance_metrics_binary(model = clf_pipeline_binary_xgboost, \n",
    "                                                                 data = X_train, \n",
    "                                                                 true_y = y_train['up_down'].values, \n",
    "                                                                 train_valid_test = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D60fFgBiIgoE"
   },
   "source": [
    "### performance evaluation & confusion matrix (valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-lT5oG2IbYK"
   },
   "outputs": [],
   "source": [
    "accuracy_score_valid, f1score_valid = performance_metrics_binary(model = clf_pipeline_binary_xgboost, \n",
    "                                                                 data = X_valid, \n",
    "                                                                 true_y = y_valid['up_down'].values, \n",
    "                                                                 train_valid_test = 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-cHEzKOIr5Y"
   },
   "source": [
    "## Build Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzn2MZF_IlzP"
   },
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()\n",
    "stack_pipeline = StackingClassifier(estimators = [('linearsvc', clf_pipeline_binary), ('xgboost', clf_pipeline_binary_xgboost)], \n",
    "                                    final_estimator = logistic)\n",
    "\n",
    "stack_pipeline.fit(X_train, y_train['up_down'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjK2ta6EI2_Y"
   },
   "source": [
    "### performance evaluation & confusion matrix (training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4phEM4NyIvrf"
   },
   "outputs": [],
   "source": [
    "accuracy_score_train, f1score_train = performance_metrics_binary(model = stack_pipeline, \n",
    "                                                                 data = X_train, \n",
    "                                                                 true_y = y_train['up_down'].values, \n",
    "                                                                 train_valid_test = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-UkE9twJF05"
   },
   "source": [
    "### performance evaluation & confusion matrix (valid data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMduybXjI655"
   },
   "outputs": [],
   "source": [
    "accuracy_score_valid, f1score_valid = performance_metrics_binary(model = stack_pipeline, \n",
    "                                                                 data = X_valid, \n",
    "                                                                 true_y = y_valid['up_down'].values, \n",
    "                                                                 train_valid_test = 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7ptur1u_bYC"
   },
   "source": [
    "### 10-fold cross validation (Monte Carlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbV0hQQiJNsY"
   },
   "outputs": [],
   "source": [
    "folds = ['fold-1', 'fold-2', 'fold-3', 'fold-4', 'fold-5', 'fold-6', 'fold-7', 'fold-8', 'fold-9', 'fold-10']\n",
    "mccv_score_dict = {'fold':[], 'train_accuracy':[], 'train_f1':[], 'valid_accuracy': [], 'valid_f1': []}\n",
    "  \n",
    "for fold in folds:\n",
    "  X_train, y_train = dict_folds_data[fold]['train']\n",
    "  X_valid, y_valid = dict_folds_data[fold]['valid']\n",
    "\n",
    "  minmaxscaler_price = MinMaxScaler()\n",
    "\n",
    "  ma_pipeline = Pipeline(steps = [('norm_price', minmaxscaler_price)])\n",
    "\n",
    "  column_transformer = ColumnTransformer(transformers = [('price_ma', ma_pipeline, ['Close', 'MA10', 'MA20', 'MA30',\n",
    "                                                                                    'appl_Close', 'msft_Close', 'amzn_Close', \n",
    "                                                                                    'nvda_Close', 'brk_Close', 'googl_Close', \n",
    "                                                                                    'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close'])])\n",
    "\n",
    "  clf_pipeline_binary = Pipeline(steps = [('preprocessing', column_transformer), \n",
    "                                          ('classification',LinearSVC(C= 0.05, class_weight = 'balanced'))])\n",
    "\n",
    "  #-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "  minmaxscaler_price_xgboost = MinMaxScaler()\n",
    "  ma_pipeline_xg = Pipeline(steps = [('norm_price', minmaxscaler_price_xgboost)])\n",
    "  column_transformer_xg = ColumnTransformer(transformers = [('price_ma', ma_pipeline_xg, ['Close', 'MA10', 'MA20', 'MA30',\n",
    "                                                                                          'appl_Close', 'msft_Close', 'amzn_Close', \n",
    "                                                                                          'nvda_Close', 'brk_Close', 'googl_Close', \n",
    "                                                                                          'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close'])])\n",
    "  clf_xgb = XGBClassifier(booster = 'gbtree', min_split_loss = 0.01, learning_rate = 0.001, max_depth = 4, n_estimators = 1000, scale_pos_weight = 0.9)\n",
    "  clf_pipeline_binary_xgboost = Pipeline(steps = [('preprocessing', column_transformer_xg), \n",
    "                                                  ('classification',clf_xgb)])\n",
    "  \n",
    "  #--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "  logistic = LogisticRegression()\n",
    "  stack_pipeline = StackingClassifier(estimators = [('linearsvc', clf_pipeline_binary), ('xgboost', clf_pipeline_binary_xgboost)], \n",
    "                                    final_estimator = logistic)\n",
    "\n",
    "  stack_pipeline.fit(X_train, y_train['up_down'].values)\n",
    "  train_acc = stack_pipeline.score(X_train, y_train['up_down'].values)\n",
    "  predicted_y_train = stack_pipeline.predict(X_train)\n",
    "  train_f1 = f1_score(y_train['up_down'].values, predicted_y_train, average = 'macro')\n",
    "  valid_acc = stack_pipeline.score(X_valid, y_valid['up_down'].values)\n",
    "  predicted_y_valid = stack_pipeline.predict(X_valid)\n",
    "  valid_f1 = f1_score(y_valid['up_down'].values, predicted_y_valid, average = 'macro')\n",
    "\n",
    "  mccv_score_dict['fold'].append(fold)\n",
    "  mccv_score_dict['train_accuracy'].append(train_acc)\n",
    "  mccv_score_dict['train_f1'].append(train_f1)\n",
    "  mccv_score_dict['valid_accuracy'].append(valid_acc)\n",
    "  mccv_score_dict['valid_f1'].append(valid_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pr3Df_x4BxMF"
   },
   "outputs": [],
   "source": [
    "df_mccv_score_stacking = pd.DataFrame(mccv_score_dict)\n",
    "df_mccv_score_stacking.loc[len(df_mccv_score_stacking)] = ['average', \n",
    "                                                           df_mccv_score_stacking['train_accuracy'].mean(), \n",
    "                                                           df_mccv_score_stacking['train_f1'].mean(), \n",
    "                                                           df_mccv_score_stacking['valid_accuracy'].mean(), \n",
    "                                                           df_mccv_score_stacking['valid_f1'].mean()]\n",
    "\n",
    "df_mccv_score_stacking.loc[len(df_mccv_score_stacking)] = ['std', \n",
    "                                                           df_mccv_score_stacking.iloc[:-1,1].std(), \n",
    "                                                           df_mccv_score_stacking.iloc[:-1,2].std(), \n",
    "                                                           df_mccv_score_stacking.iloc[:-1,3].std(), \n",
    "                                                           df_mccv_score_stacking.iloc[:-1,4].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24utY3woBy2P"
   },
   "outputs": [],
   "source": [
    "df_mccv_score_stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0XDelVzDHEf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 6))\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "ax1.plot(df_mccv_score_stacking.iloc[:-2,0], df_mccv_score_stacking.iloc[:-2,1], label = 'train_accuracy', color = 'black', linewidth = 0.5)\n",
    "ax1.axhline(df_mccv_score_stacking.iloc[10, 1], label = 'avg train_accuracy', color = 'black', linewidth = 0.2, linestyle = '--')\n",
    "ax1.set_ylim((0.3, 0.6))\n",
    "ax1.tick_params(labelbottom=False)\n",
    "ax1.legend()\n",
    "ax2 = plt.subplot(2, 2, 3)\n",
    "ax2.plot(df_mccv_score_stacking.iloc[:-2,0], df_mccv_score_stacking.iloc[:-2,3], label = 'valid_accuracy', color = 'black', linewidth = 0.5)\n",
    "ax2.axhline(df_mccv_score_stacking.iloc[10, 3], label = 'avg valid_accuracy', color = 'black', linewidth = 0.2, linestyle = '--')\n",
    "ax2.set_ylim((0.3, 0.6))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.legend()\n",
    "ax3 = plt.subplot(2, 2, 2)\n",
    "ax3.plot(df_mccv_score_stacking.iloc[:-2,0], df_mccv_score_stacking.iloc[:-2,2], label = 'train_f1', color = 'red', linewidth = 0.5)\n",
    "ax3.axhline(df_mccv_score_stacking.iloc[10, 2], label = 'avg train_f1', color = 'red', linewidth = 0.2, linestyle = '--')\n",
    "ax3.set_ylim((0.1, 0.4))\n",
    "ax3.tick_params(labelbottom=False)\n",
    "plt.legend()\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "ax4.plot(df_mccv_score_stacking.iloc[:-2,0], df_mccv_score_stacking.iloc[:-2,4], label = 'valid_f1', color = 'red', linewidth = 0.5)\n",
    "ax4.axhline(df_mccv_score_stacking.iloc[10, 4], label = 'avg valid_f1', color = 'red', linewidth = 0.2, linestyle = '--')\n",
    "ax4.set_ylim((0.1, 0.4))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytSmH5nrFwKB"
   },
   "source": [
    "## Stacking Models : performace evaluation & visualization (train on training & valid dataset and test on test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-ARX2J9FAUR"
   },
   "outputs": [],
   "source": [
    "X_all_train, X_test, y_all_train, y_test = load_dataset(sp500_fn = load_sp500_data, training_data = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yORjw7RSF0ei"
   },
   "outputs": [],
   "source": [
    "mccv_score_dict_all = {'train_accuracy':[], 'train_f1':[], 'test_accuracy': [], 'test_f1': []}\n",
    "\n",
    "minmaxscaler_price = MinMaxScaler()\n",
    "\n",
    "ma_pipeline = Pipeline(steps = [('norm_price', minmaxscaler_price)])\n",
    "\n",
    "column_transformer = ColumnTransformer(transformers = [('price_ma', ma_pipeline, ['Close', 'MA10', 'MA20', 'MA30',\n",
    "                                                                                  'appl_Close', 'msft_Close', 'amzn_Close', \n",
    "                                                                                  'nvda_Close', 'brk_Close', 'googl_Close', \n",
    "                                                                                  'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close'])])\n",
    "\n",
    "clf_pipeline_binary = Pipeline(steps = [('preprocessing', column_transformer), \n",
    "                                        ('classification',LinearSVC(C= 0.05, class_weight = 'balanced'))])\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "minmaxscaler_price_xgboost = MinMaxScaler()\n",
    "ma_pipeline_xg = Pipeline(steps = [('norm_price', minmaxscaler_price_xgboost)])\n",
    "column_transformer_xg = ColumnTransformer(transformers = [('price_ma', ma_pipeline_xg, ['Close', 'MA10', 'MA20', 'MA30',\n",
    "                                                                                        'appl_Close', 'msft_Close', 'amzn_Close', \n",
    "                                                                                        'nvda_Close', 'brk_Close', 'googl_Close', \n",
    "                                                                                        'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close'])])\n",
    "clf_xgb = XGBClassifier(booster = 'gbtree', min_split_loss = 0.01, learning_rate = 0.001, max_depth = 4, n_estimators = 1000, scale_pos_weight = 0.9)\n",
    "clf_pipeline_binary_xgboost = Pipeline(steps = [('preprocessing', column_transformer_xg), \n",
    "                                                ('classification',clf_xgb)])\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "stack_pipeline = StackingClassifier(estimators = [('linearsvc', clf_pipeline_binary), ('xgboost', clf_pipeline_binary_xgboost)], \n",
    "                                  final_estimator = logistic)\n",
    "\n",
    "stack_pipeline.fit(X_all_train, y_all_train['up_down'].values)\n",
    "train_all_acc = stack_pipeline.score(X_all_train, y_all_train['up_down'].values)\n",
    "predicted_y_all_train = stack_pipeline.predict(X_all_train)\n",
    "train_all_f1 = f1_score(y_all_train['up_down'].values, predicted_y_all_train, average = 'macro')\n",
    "test_acc = stack_pipeline.score(X_test, y_test['up_down'].values)\n",
    "predicted_y_test = stack_pipeline.predict(X_test)\n",
    "test_f1 = f1_score(y_test['up_down'].values, predicted_y_test, average = 'macro')\n",
    "\n",
    "mccv_score_dict_all['train_accuracy'].append(train_all_acc)\n",
    "mccv_score_dict_all['train_f1'].append(train_all_f1)\n",
    "mccv_score_dict_all['test_accuracy'].append(test_acc)\n",
    "mccv_score_dict_all['test_f1'].append(test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0N1OgElLGKZ"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(mccv_score_dict_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIS45ohbOETI"
   },
   "source": [
    "### performance evaluation & confusion matrix (train & valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGPYz_kHNO_r"
   },
   "outputs": [],
   "source": [
    "accuracy_score_valid, f1score_valid = performance_metrics_binary(model = stack_pipeline, \n",
    "                                                                 data = X_all_train, \n",
    "                                                                 true_y = y_all_train['up_down'].values, \n",
    "                                                                 train_valid_test = 'train_valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uod60Q_1OJid"
   },
   "source": [
    "### performance evalution & confusion matrix (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxKuY1OgNzXr"
   },
   "outputs": [],
   "source": [
    "accuracy_score_valid, f1score_valid = performance_metrics_binary(model = stack_pipeline, \n",
    "                                                                 data = X_test, \n",
    "                                                                 true_y = y_test['up_down'].values, \n",
    "                                                                 train_valid_test = 'test')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
