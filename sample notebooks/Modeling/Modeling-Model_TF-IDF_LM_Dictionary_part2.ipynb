{"cells":[{"cell_type":"markdown","metadata":{"id":"HDHmsKfaPbiQ"},"source":["## Install/ Import Library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46rxgJm7ODch"},"outputs":[],"source":["! pip install -U spacy\n","! python -m spacy download en_core_web_lg\n","! pip install yfinance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwTRkhSwPgDA"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import seaborn as sns\n","import re\n","from scipy.sparse import csr_matrix\n","\n","import spacy\n","from tqdm.auto import tqdm\n","from pathlib import Path\n","import os\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, OneHotEncoder\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n","from sklearn.model_selection import ParameterGrid\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.linear_model import LogisticRegression\n","import pickle\n","\n","from xgboost import XGBClassifier\n","\n","import yfinance\n"]},{"cell_type":"markdown","metadata":{"id":"59addqYftrr6"},"source":["## Helper Functions"]},{"cell_type":"markdown","metadata":{"id":"x3V3A_TqxJfu"},"source":["### load sp500 data & prepare technical index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cgr6qJMwCR2"},"outputs":[],"source":["def load_sp500_data(start_date = '2014-12-31', end_date = '2021-01-05'):\n","  sp = yfinance.Ticker('^GSPC')\n","  sp_history = sp.history(start =start_date, end = end_date)\n","  sp_history.reset_index(inplace = True)\n","  sp_history['Date'] = pd.to_datetime(sp_history['Date'].dt.date)\n","  sp_history.drop(labels = ['Dividends', 'Stock Splits'], axis = 1, inplace = True)\n","  sp_history = sp_history.loc[:,['Date', 'Open', 'High', 'Low', 'Volume', 'Close']]\n","  sp_history['Close+1day'] = sp_history['Close'].shift(-1)\n","  sp_history.dropna(inplace = True)\n","  sp_history['up_down'] = sp_history[['Close', 'Close+1day']].apply(lambda x: 1 if x['Close+1day'] > x['Close'] else 0, axis = 1)\n","\n","  appl = yfinance.Ticker('AAPL')\n","  appl_history = appl.history(start =start_date, end = end_date)\n","  appl_history.reset_index(inplace = True)\n","  appl_history['Date'] = pd.to_datetime(appl_history['Date'].dt.date)\n","  appl_history['Close'] = appl_history['Close'].pct_change()\n","\n","  msft = yfinance.Ticker('MSFT')\n","  msft_history = msft.history(start =start_date, end = end_date)\n","  msft_history.reset_index(inplace = True)\n","  msft_history['Date'] = pd.to_datetime(msft_history['Date'].dt.date)\n","  msft_history['Close'] = msft_history['Close'].pct_change()\n","\n","  amzn = yfinance.Ticker('AMZN')\n","  amzn_history = amzn.history(start =start_date, end = end_date)\n","  amzn_history.reset_index(inplace = True)\n","  amzn_history['Date'] = pd.to_datetime(amzn_history['Date'].dt.date)\n","  amzn_history['Close'] = amzn_history['Close'].pct_change()\n","\n","  nvda = yfinance.Ticker('NVDA')\n","  nvda_history = nvda.history(start =start_date, end = end_date)\n","  nvda_history.reset_index(inplace = True)\n","  nvda_history['Date'] = pd.to_datetime(nvda_history['Date'].dt.date)\n","  nvda_history['Close'] = nvda_history['Close'].pct_change()\n","\n","  brk = yfinance.Ticker('BRK-B')\n","  brk_history = brk.history(start =start_date, end = end_date)\n","  brk_history.reset_index(inplace = True)\n","  brk_history['Date'] = pd.to_datetime(brk_history['Date'].dt.date)\n","  brk_history['Close'] = brk_history['Close'].pct_change()\n","\n","  googl = yfinance.Ticker('GOOGL')\n","  googl_history = googl.history(start =start_date, end = end_date)\n","  googl_history.reset_index(inplace = True)\n","  googl_history['Date'] = pd.to_datetime(googl_history['Date'].dt.date)\n","  googl_history['Close'] = googl_history['Close'].pct_change()\n","\n","  tsla = yfinance.Ticker('TSLA')\n","  tsla_history = tsla.history(start =start_date, end = end_date)\n","  tsla_history.reset_index(inplace = True)\n","  tsla_history['Date'] = pd.to_datetime(tsla_history['Date'].dt.date)\n","  tsla_history['Close'] = tsla_history['Close'].pct_change()\n","\n","  meta = yfinance.Ticker('META')\n","  meta_history = meta.history(start =start_date, end = end_date)\n","  meta_history.reset_index(inplace = True)\n","  meta_history['Date'] = pd.to_datetime(meta_history['Date'].dt.date)\n","  meta_history['Close'] = meta_history['Close'].pct_change()\n","\n","  xom = yfinance.Ticker('XOM')\n","  xom_history = xom.history(start =start_date, end = end_date)\n","  xom_history.reset_index(inplace = True)\n","  xom_history['Date'] = pd.to_datetime(xom_history['Date'].dt.date)\n","  xom_history['Close'] = xom_history['Close'].pct_change()\n","\n","  jpm = yfinance.Ticker('JPM')\n","  jpm_history = jpm.history(start =start_date, end = end_date)\n","  jpm_history.reset_index(inplace = True)\n","  jpm_history['Date'] = pd.to_datetime(jpm_history['Date'].dt.date)\n","  jpm_history['Close'] = jpm_history['Close'].pct_change()\n","\n","  for i in [10, 20, 30]:\n","    sp_history['MA'+str(i)] = sp_history['Close'].rolling(i).mean()\n","\n","  for df, name in zip([appl_history, msft_history, amzn_history, nvda_history, brk_history, \n","                       googl_history, tsla_history, meta_history, xom_history, jpm_history], \n","                      ['appl_Close', 'msft_Close', 'amzn_Close', 'nvda_Close', 'brk_Close', \n","                       'googl_Close', 'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close']):\n","    df = df[['Date', 'Close']].rename(columns = {'Close': name})\n","    sp_history = sp_history.merge(df, left_on = 'Date', right_on = 'Date', how = 'left')\n","  \n","  sp_history = sp_history[(sp_history['Date'] >= '2014-12-31') & (sp_history['Date'] <= '2020-12-31')].copy()\n","  sp_history.dropna(inplace = True)\n","\n","  return sp_history"]},{"cell_type":"markdown","metadata":{"id":"cWPI2zx28tz9"},"source":["### tokenize & clean data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MiX7phD8z4i"},"outputs":[],"source":["def tokenization_clean(text: 'str', nlp_model: 'spacy_model'):\n","  doc = nlp_model(text)\n","  tok_aft_spacy = [re.sub(r'[^\\w\\s]', '', tok.lemma_.lower()) for tok in doc \n","                   if not tok.is_stop\n","                   and not tok.is_punct \n","                   and not tok.like_num \n","                   and not tok.like_url \n","                   and not tok.is_space \n","                   and not tok.like_email \n","                   and not tok.is_left_punct \n","                   and not tok.is_right_punct \n","                   and not tok.is_digit \n","                   and not tok.is_currency]\n","  \n","  join_tok_aft_spacy = ' '.join(tok_aft_spacy)\n","  return join_tok_aft_spacy"]},{"cell_type":"markdown","metadata":{"id":"IMUZcpjm83RU"},"source":["### load text dataset & combine sp500"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5nXLRtY8-CF"},"outputs":[],"source":["def load_fold_dataset(directory: 'str', \n","                      file_name: 'str',\n","                      sp500_fn: 'function', \n","                      fn_tokenization_clean: 'function', \n","                      nlp_model):\n","  \n","  tqdm.pandas()\n","  path = Path(directory)\n","  file_path = path / file_name\n","  data = pd.read_csv(file_path)\n","  data = data[['timestamp', 'text']].groupby('timestamp', as_index = False).agg({'text':' '.join})\n","  sp500 = sp500_fn()\n","\n","  data['tokenized_text'] = data['text'].progress_apply(lambda x: fn_tokenization_clean(x, nlp_model))\n","  data['timestamp'] = pd.to_datetime(data['timestamp'])\n","\n","  column_subset_X = ['Date','Close'] + list(sp500.columns[7:]) + ['tokenized_text']\n","\n","  data_merge = sp500.merge(data[['timestamp', 'tokenized_text']], how = 'left', left_on = 'Date', right_on = 'timestamp')\n","  data_merge = data_merge.dropna()\n","  data_merge_X = data_merge.loc[:,column_subset_X].copy()\n","  data_merge_y = data_merge.loc[:,['Date', 'up_down']]\n","\n","  return data_merge_X, data_merge_y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7B3lCKlW629r"},"outputs":[],"source":["def load_dataset(directory: 'str', \n","                 file_name: 'str', \n","                 sp500_fn: 'function',\n","                 fn_tokenization_clean: 'function', \n","                 nlp_model, \n","                 training_data = True):\n","  \n","  tqdm.pandas()\n","  path = Path(directory)\n","  file_path = path / file_name\n","\n","  with open(file_path, 'r', encoding=\"utf-8\") as f:\n","    data = eval(f.read())\n","\n","  data = pd.DataFrame({'timestamp':list(data.keys()), 'text': list(data.values())})\n","  sp500 = sp500_fn()\n","\n","  data['tokenized_text'] = data['text'].progress_apply(lambda x: fn_tokenization_clean(x, nlp_model))\n","  data['timestamp'] = pd.to_datetime(data['timestamp'])\n","\n","  column_subset_X = ['Date','Close'] + list(sp500.columns[7:]) + ['tokenized_text']\n","\n","  data_merge = sp500.merge(data, how = 'left', left_on = 'Date', right_on = 'timestamp')\n","  data_merge = data_merge.dropna()\n","  data_merge_X = data_merge.loc[:,column_subset_X].copy()\n","  data_merge_y = data_merge.loc[:,['Date', 'up_down']]\n","  \n","  if training_data == True:\n","    X_train = data_merge_X[data_merge_X['Date']<='2018-12-31'].copy()\n","    X_valid = data_merge_X[data_merge_X['Date']>='2019-01-01'].copy()\n","    y_train = data_merge_y[data_merge_y['Date']<='2018-12-31'].copy()\n","    y_valid = data_merge_y[data_merge_y['Date']>='2019-01-01'].copy()\n","    return X_train, X_valid, y_train, y_valid\n","  \n","  elif training_data == False:\n","    return data_merge_X, data_merge_y"]},{"cell_type":"markdown","metadata":{"id":"mdTo-yrj50_d"},"source":["### function to prepare 10 folds data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCnfqzop57Vw"},"outputs":[],"source":["def prepare_folds_dataset(cv_path: 'str'):\n","  cv_path = Path(cv_path)\n","  fold_dataset = {'fold-1':{'train':None, 'valid':None}, \n","                  'fold-2':{'train':None, 'valid':None}, \n","                  'fold-3':{'train':None, 'valid':None}, \n","                  'fold-4':{'train':None, 'valid':None}, \n","                  'fold-5':{'train':None, 'valid':None}, \n","                  'fold-6':{'train':None, 'valid':None}, \n","                  'fold-7':{'train':None, 'valid':None}, \n","                  'fold-8':{'train':None, 'valid':None}, \n","                  'fold-9':{'train':None, 'valid':None}, \n","                  'fold-10':{'train':None, 'valid':None}}\n","\n","  for i in cv_path.iterdir():\n","    i.name\n","    for j in i.iterdir():\n","      if 'train' in j.name:\n","        train_path = j\n","\n","        X_fold_train, y_fold_train = load_fold_dataset(directory = i, \n","                                            file_name = train_path.name,\n","                                            sp500_fn = load_sp500_data,\n","                                            fn_tokenization_clean = tokenization_clean,\n","                                            nlp_model = nlp_model)\n","        fold_dataset[i.name]['train'] = (X_fold_train, y_fold_train)\n","      elif 'valid' in j.name:\n","        valid_path = j\n","\n","        X_fold_valid, y_fold_valid = load_fold_dataset(directory = i,\n","                                            file_name = valid_path.name,\n","                                            sp500_fn = load_sp500_data,\n","                                            fn_tokenization_clean = tokenization_clean,\n","                                            nlp_model = nlp_model)\n","        fold_dataset[i.name]['valid'] = (X_fold_valid, y_fold_valid)\n","  \n","  return fold_dataset"]},{"cell_type":"markdown","metadata":{"id":"p_FqqPEU9PxM"},"source":["### load loughran_mcdonal sentiment dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfQORc9JtrPo"},"outputs":[],"source":["def loughran_mcdonald_dict(directory: 'str', file_name: 'str'):\n","  path = Path(directory)\n","  file_path = path / file_name\n","  data = pd.read_csv(file_path)\n","\n","  data_part = data[['Word', 'Negative', 'Positive', \n","                    'Uncertainty', 'Litigious', 'Strong_Modal', \n","                    'Weak_Modal', 'Constraining']].copy()\n","\n","  data_part['Word'] = data_part['Word'].str.lower()\n","\n","  for i in list(data_part.columns)[1:]:\n","    data_part[i] = data_part[i].apply(lambda x: 1 if x >0 else 0)\n","  \n","  data_part.drop(index = 50741, inplace = True) # drop nan values\n","  data_part.reset_index(drop = True, inplace = True)\n","\n","  return data_part"]},{"cell_type":"markdown","metadata":{"id":"5V85nS809Yeo"},"source":["### multiplication of doc-term maxtrix and term-sentiment matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Il-mv9769yQ1"},"outputs":[],"source":["def combine_count_sent(data, \n","                       loughran_mcdonald_dict_fn = loughran_mcdonald_dict, \n","                       directory = '../data/TF-IDF Models', \n","                       file_name = 'Loughran-McDonald_MasterDictionary_1993-2021.csv'):\n","  \n","  sent_dict = loughran_mcdonald_dict_fn(directory, file_name)\n","  sparse_sent_dict = csr_matrix(sent_dict.iloc[:,1:].values)\n","  news_sentiment = (data*sparse_sent_dict)\n","\n","  return news_sentiment.toarray()"]},{"cell_type":"markdown","metadata":{"id":"vlZ3y8Hl9GgZ"},"source":["### gridsearchcv, using 10 folds dataset (Linear SVC)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVDXIfSD9dvS"},"outputs":[],"source":["def gridsearchcv(folds_data: 'dict', parametergrid: 'list', model: 'function'):\n","  folds = ['fold-1', 'fold-2', 'fold-3', 'fold-4', 'fold-5', 'fold-6', 'fold-7', 'fold-8', 'fold-9', 'fold-10']\n","  score_dict = {'parameter': [], 'fold':[], 'train_accuracy':[], 'train_f1':[], 'valid_accuracy': [], 'valid_f1': []}\n","\n","  for parameter in tqdm(parametergrid):\n","    model_initialized = model.set_params(preprocessing__tfidf__tfidf_vectorizer__max_features = parameter['preprocessing__tfidf__tfidf_vectorizer__max_features'], \n","                                         classification__C = parameter['classification__C'], \n","                                         classification__penalty = parameter['classification__penalty'])\n","    \n","    for fold in folds:\n","      score_dict['parameter'].append(parameter)\n","      X_train, y_train = folds_data[fold]['train']\n","      X_valid, y_valid = folds_data[fold]['valid']\n","      model_initialized.fit(X_train, y_train['up_down'].values)\n","      train_acc = model_initialized.score(X_train, y_train['up_down'].values)\n","      predicted_y_train = model_initialized.predict(X_train)\n","      train_f1 = f1_score(y_train['up_down'].values, predicted_y_train, average = 'macro')\n","      valid_acc = model_initialized.score(X_valid, y_valid['up_down'].values)\n","      predicted_y_valid = model_initialized.predict(X_valid)\n","      valid_f1 = f1_score(y_valid['up_down'].values, predicted_y_valid, average = 'macro')\n","\n","      score_dict['fold'].append(fold)\n","      score_dict['train_accuracy'].append(train_acc)\n","      score_dict['train_f1'].append(train_f1)\n","      score_dict['valid_accuracy'].append(valid_acc)\n","      score_dict['valid_f1'].append(valid_f1)\n","  \n","  return score_dict\n"]},{"cell_type":"markdown","metadata":{"id":"rdvT2iW_2uo8"},"source":["### function to visualize gridsearchcv scores (linearSVC)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ME7nYySu2tk1"},"outputs":[],"source":["def visualize_gridsearchcv(cv_result: 'sklearn_gridsearch_model'):\n","  features_list = [2000, 4000, 6000, 8000, 10000, 12000]\n","  color = ['#3498DB']*len(features_list)\n","  \n","  df_cv_result = cv_result[['classification__C', \n","                            'preprocessing__tfidf__tfidf_vectorizer__max_features', \n","                            'valid_f1']].copy()\n","                               \n","  df_cv_result.rename(columns = {'classification__C': 'linearSVC_C', \n","                                 'preprocessing__tfidf__tfidf_vectorizer__max_features': 'tfidf_feature'}, \n","                      inplace = True)\n","  \n","  plt.figure(figsize = (8, 6))\n","\n","  for idx, feature_no, color in zip(range(len(features_list)), features_list, color):\n","    ax = plt.subplot(6, 1, idx+1)\n","    ax.plot(df_cv_result.loc[(df_cv_result['tfidf_feature'] == feature_no), 'linearSVC_C'], \n","            df_cv_result.loc[(df_cv_result['tfidf_feature'] == feature_no), 'valid_f1'],\n","            color = color, \n","            linewidth = 1, \n","            label = f'tfidf_feature number: {feature_no}')\n","    \n","    #ax.set_ylim((0., 0.62))\n","    ax.set_ylabel('mean f1', fontsize = 8)\n","    plt.yticks(fontsize=8)\n","    ax.legend(fontsize = 8)\n","\n","    if idx != len(features_list) - 1:\n","      ax.tick_params(labelbottom=False)\n","    \n","    if idx == 0:\n","      plt.title('validation f1 score of the grid search cv results')\n","    \n","    \n","\n","  plt.xlabel('linear SVC, C value')\n","  \n","  plt.tight_layout(pad = 0.2)\n","\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zAG43EdAtAGQ"},"source":["### gridsearchcv, using 10 folds dataset (XGBoost)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nt1auF4dtENq"},"outputs":[],"source":["def gridsearchcv_xgb(folds_data: 'dict', parametergrid: 'list', model: 'function'):\n","  folds = ['fold-1', 'fold-2', 'fold-3', 'fold-4', 'fold-5', 'fold-6', 'fold-7', 'fold-8', 'fold-9', 'fold-10']\n","  score_dict = {'parameter': [], 'fold':[], 'train_accuracy':[], 'train_f1':[], 'valid_accuracy': [], 'valid_f1': []}\n","\n","  for parameter in tqdm(parametergrid):\n","    model_initialized = model.set_params(classification__learning_rate = parameter['classification__learning_rate'], \n","                                         classification__max_depth = parameter['classification__max_depth'])\n","    \n","    for fold in folds:\n","      score_dict['parameter'].append(parameter)\n","      X_train, y_train = folds_data[fold]['train']\n","      X_valid, y_valid = folds_data[fold]['valid']\n","      model_initialized.fit(X_train, y_train['up_down'].values)\n","      train_acc = model_initialized.score(X_train, y_train['up_down'].values)\n","      predicted_y_train = model_initialized.predict(X_train)\n","      train_f1 = f1_score(y_train['up_down'].values, predicted_y_train, average = 'macro')\n","      valid_acc = model_initialized.score(X_valid, y_valid['up_down'].values)\n","      predicted_y_valid = model_initialized.predict(X_valid)\n","      valid_f1 = f1_score(y_valid['up_down'].values, predicted_y_valid, average = 'macro')\n","\n","      score_dict['fold'].append(fold)\n","      score_dict['train_accuracy'].append(train_acc)\n","      score_dict['train_f1'].append(train_f1)\n","      score_dict['valid_accuracy'].append(valid_acc)\n","      score_dict['valid_f1'].append(valid_f1)\n","  \n","  return score_dict"]},{"cell_type":"markdown","metadata":{"id":"jEQ1Xyt6c-89"},"source":["### function to visualize gridsearchcv result (XGBoost)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ovbq8GIjdHg9"},"outputs":[],"source":["def visualize_gridsearchcv_xgb(cv_result: 'sklearn gridsearch model'):\n","  depth_list = [3, 4, 5]\n","  color = ['#0099FF', '#2D88C5', '#3F7090']\n","  \n","  df_cv_result = cv_result[['classification__learning_rate',\n","                            'classification__max_depth',\n","                            'valid_f1']].copy()\n","                               \n","  df_cv_result.rename(columns = {'classification__learning_rate': 'learning_rate', \n","                                 'classification__max_depth': 'max_depth'}, \n","                      inplace = True)\n","  \n","  plt.figure(figsize = (8, 6))\n","\n","  for depth_no, color in zip(depth_list, color):\n","    plt.plot(df_cv_result.loc[(df_cv_result['max_depth'] == depth_no), 'learning_rate'], \n","             df_cv_result.loc[(df_cv_result['max_depth'] == depth_no), 'valid_f1'], \n","             color = color, label = f'max_depth: {depth_no}')\n","    \n","\n","  plt.xlabel('XGBoost, learning rate')\n","  plt.ylabel('mean valid f1')\n","  plt.tight_layout(pad = 0.5)\n","  plt.legend()\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"899zwDcu-OLa"},"source":["### function to visualize confusion matrix (binary label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8ShYoZu-N1s"},"outputs":[],"source":["def performance_metrics_binary(model, data, true_y, train_valid_test: 'str'):\n","  predicted_y = model.predict(data)\n","  accuracy_score = model.score(data, true_y)\n","  f1score = f1_score(true_y, predicted_y, average = 'macro')\n","  \n","\n","  plt.figure(figsize = (4, 4))\n","  sns.set(font_scale=1.2)\n","  cm_result = confusion_matrix(true_y, predicted_y, normalize = 'pred')\n","\n","  confusion_matrix_result_heatmap = sns.heatmap(cm_result, \n","                                                cmap=\"Blues\", \n","                                                annot = True, \n","                                                fmt=\".2f\", annot_kws={'size': 15}, \n","                                                xticklabels=['Negative', 'Positive'], \n","                                                yticklabels=['Negative', 'Positive'])\n","\n","  confusion_matrix_result_heatmap.set(xlabel='Predicted Label', ylabel='True Label', title = 'price movement')\n","\n","  plt.show()\n","  print(f'\\n{train_valid_test} accuracy: {accuracy_score}, {train_valid_test} f1 score: {f1score}')\n","  return accuracy_score, f1score"]},{"cell_type":"markdown","metadata":{"id":"O0_yDJy6R44J"},"source":["## Load Data"]},{"cell_type":"markdown","metadata":{"id":"CmEnwmRp_rIm"},"source":["### initialize spacy model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ME8CJNjDtVRq"},"outputs":[],"source":["nlp_model = spacy.load('en_core_web_lg')"]},{"cell_type":"markdown","metadata":{"id":"4PSnxwQUASvn"},"source":["### prepare loughran-mcdonald sentiment dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDnU5ckbAX5f"},"outputs":[],"source":["lm_sent_dict = loughran_mcdonald_dict(directory = '../data/TF-IDF Models', \n","                                      file_name = 'Loughran-McDonald_MasterDictionary_1993-2021.csv')"]},{"cell_type":"markdown","metadata":{"id":"eD4njH2_72P9"},"source":["### load 10 folds dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dT8nq2l-71PV"},"outputs":[],"source":["folds_data_path = Path(r'../data/TF-IDF Models/Intermediate Output/dict_folds_data.pickle')\n","\n","if folds_data_path.is_file():\n","  with open(folds_data_path, 'rb') as f_1:\n","    dict_folds_data = pickle.load(f_1)\n","\n","else:\n","  dict_folds_data = prepare_folds_dataset(cv_path = r'../data/TF-IDF Models/Cross Validation_fold_data')"]},{"cell_type":"markdown","metadata":{"id":"DTLfSx9JSZIw"},"source":["## Build LinearSVC Pipeline"]},{"cell_type":"markdown","metadata":{"id":"djmOu2a6BDWm"},"source":["### initialize necessary functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLugrZA572n1"},"outputs":[],"source":["lm_countvectorizer = CountVectorizer(vocabulary = lm_sent_dict ['Word'])\n","tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, 2), max_features = 10000)\n","minmaxscaler = MinMaxScaler()\n","minmaxscaler_price = MinMaxScaler()\n","fn_combine = FunctionTransformer(combine_count_sent)\n","fn_transform_sparse = FunctionTransformer(csr_matrix)"]},{"cell_type":"markdown","metadata":{"id":"ij19E1mPVCiy"},"source":["### build pipeline (binary label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05mtNKsqQXrc"},"outputs":[],"source":["tfidf_pipline = Pipeline(steps = [('tfidf_vectorizer', tfidf_vectorizer)])\n","\n","lm_pipeline = Pipeline(steps = [('count_vectorizer', lm_countvectorizer), \n","                                ('matrix_mul', fn_combine), \n","                                ('norm', minmaxscaler), \n","                                ('sparse_matrix', fn_transform_sparse)])\n","\n","ma_pipeline = Pipeline(steps = [('norm_price', minmaxscaler_price)])\n","\n","column_transformer = ColumnTransformer(transformers = [('price_ma', ma_pipeline, ['Close', 'MA10', 'MA20', 'MA30',\n","                                                                                  'appl_Close', 'msft_Close', 'amzn_Close', \n","                                                                                  'nvda_Close', 'brk_Close', 'googl_Close', \n","                                                                                  'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close']), \n","                                                       ('tfidf', tfidf_pipline, 'tokenized_text'),\n","                                                       ('lm_count', lm_pipeline, 'tokenized_text')])\n","\n","clf_pipeline_binary = Pipeline(steps = [('preprocessing', column_transformer), \n","                                        ('classification',LinearSVC(C= 0.05, class_weight = 'balanced'))])"]},{"cell_type":"markdown","metadata":{"id":"PbcPn6uTRoKW"},"source":["### GridSearchCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OM87jIhTEluQ"},"outputs":[],"source":["parameters_grid = {'preprocessing__tfidf__tfidf_vectorizer__max_features':[2000, 4000, 6000, 8000, 10000, 12000],\n","                   'classification__C': [0.001, 0.005, 0.01, 0.05], \n","                   'classification__penalty': ['l2']}\n","\n","parameters = ParameterGrid(parameters_grid)\n","\n","result = gridsearchcv(folds_data = dict_folds_data, parametergrid = parameters, model = clf_pipeline_binary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9t1nOSFNbuDy"},"outputs":[],"source":["parameters_grid_1 = {'preprocessing__tfidf__tfidf_vectorizer__max_features':[],\n","                   'classification__C': [], \n","                   'classification__penalty': []}\n","\n","df_result = pd.DataFrame(result)\n","\n","for parameter in df_result['parameter']:\n","  for key, value in parameter.items():\n","    parameters_grid_1[key].append(value)\n","\n","df_result_1 = df_result.merge(pd.DataFrame(parameters_grid_1), how = 'left', left_index = True, right_index = True)\n","df_result_complete = df_result_1.iloc[:,1:].groupby(['preprocessing__tfidf__tfidf_vectorizer__max_features', \n","                                                     'classification__C',\n","                                                     'classification__penalty'], as_index = False).mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMCNd2-42Y5u"},"outputs":[],"source":["visualize_gridsearchcv(df_result_complete)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaLq4I8VgfqJ"},"outputs":[],"source":["# the best combination of parameters\n","df_result_complete.iloc[df_result_complete['valid_f1'].idxmax()]"]},{"cell_type":"markdown","metadata":{"id":"F6JkSIGA4W-Q"},"source":["### confirm best model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vn751uitqhdt"},"outputs":[],"source":["X_train, X_valid, y_train, y_valid = load_dataset(directory = '../data/TF-IDF Models/News Article Text File_Agg Daily', \n","                                                  file_name = 'articles_2015_2019_concated_summaries.txt', \n","                                                  sp500_fn = load_sp500_data,\n","                                                  fn_tokenization_clean = tokenization_clean,\n","                                                  nlp_model = nlp_model, \n","                                                  training_data = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxDZ5sGM4Wgo"},"outputs":[],"source":["lm_countvectorizer = CountVectorizer(vocabulary = lm_sent_dict ['Word'])\n","tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, 2), max_features = 4000)\n","minmaxscaler = MinMaxScaler()\n","minmaxscaler_price = MinMaxScaler()\n","fn_combine = FunctionTransformer(combine_count_sent)\n","fn_transform_sparse = FunctionTransformer(csr_matrix)\n","\n","tfidf_pipline = Pipeline(steps = [('tfidf_vectorizer', tfidf_vectorizer)])\n","\n","lm_pipeline = Pipeline(steps = [('count_vectorizer', lm_countvectorizer), \n","                                ('matrix_mul', fn_combine), \n","                                ('norm', minmaxscaler), \n","                                ('sparse_matrix', fn_transform_sparse)])\n","\n","ma_pipeline = Pipeline(steps = [('norm_price', minmaxscaler_price)])\n","\n","column_transformer = ColumnTransformer(transformers = [('price_ma', ma_pipeline, ['Close', 'MA10', 'MA20', 'MA30',\n","                                                                                  'appl_Close', 'msft_Close', 'amzn_Close', \n","                                                                                  'nvda_Close', 'brk_Close', 'googl_Close', \n","                                                                                  'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close']), \n","                                                       ('tfidf', tfidf_pipline, 'tokenized_text'),\n","                                                       ('lm_count', lm_pipeline, 'tokenized_text')])\n","\n","clf_pipeline_binary = Pipeline(steps = [('preprocessing', column_transformer), \n","                                        ('classification',LinearSVC(C= 0.05, class_weight = 'balanced'))])\n","\n","clf_pipeline_binary.fit(X_train, y_train['up_down'].values)"]},{"cell_type":"markdown","metadata":{"id":"j-0Nvs4mYyZe"},"source":["### performance evaluation & confusion matrix (train dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQiJyhTLyraD"},"outputs":[],"source":["accuracy_score_train, f1score_train = performance_metrics_binary(model = clf_pipeline_binary, \n","                                                                 data = X_train, \n","                                                                 true_y = y_train['up_down'].values, \n","                                                                 train_valid_test = 'train')"]},{"cell_type":"markdown","metadata":{"id":"j6QpIeQwRsSd"},"source":["### performance evaluation & confusion matrix (valid dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyXrQmpZ5LZB"},"outputs":[],"source":["accuracy_score_valid, f1score_valid = performance_metrics_binary(model = clf_pipeline_binary, \n","                                                                 data = X_valid, \n","                                                                 true_y = y_valid['up_down'].values, \n","                                                                 train_valid_test = 'valid')"]},{"cell_type":"markdown","metadata":{"id":"4xFpVvqNFTmg"},"source":["## Build XGBoost Classifier"]},{"cell_type":"markdown","metadata":{"id":"9F6XsLANFds_"},"source":["### initialize necessay functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6TQWwhmJFcCd"},"outputs":[],"source":["lm_countvectorizer_xgboost = CountVectorizer(vocabulary = lm_sent_dict ['Word'])\n","tfidf_vectorizer_xgboost = TfidfVectorizer(ngram_range = (1, 2), max_features = 10000)\n","minmaxscaler_lm_xgboost = MinMaxScaler()\n","minmaxscaler_price_xgboost = MinMaxScaler()\n","fn_combine_xgboost = FunctionTransformer(combine_count_sent)\n","fn_transform_sparse_xgboost = FunctionTransformer(csr_matrix)\n","clf_xgb = XGBClassifier(booster = 'gbtree', tree_method='gpu_hist', min_split_loss = 0.01, learning_rate = 0.01, max_depth = 3, n_estimators = 1000, scale_pos_weight = 0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aCj-uZZUKTG7"},"outputs":[],"source":["tfidf_pipline_xg = Pipeline(steps = [('tfidf_vectorizer', tfidf_vectorizer_xgboost)])\n","\n","lm_pipeline_xg = Pipeline(steps = [('count_vectorizer', lm_countvectorizer_xgboost), \n","                                   ('matrix_mul', fn_combine_xgboost), \n","                                   ('norm', minmaxscaler_lm_xgboost), \n","                                   ('sparse_matrix', fn_transform_sparse_xgboost)])\n","\n","ma_pipeline_xg = Pipeline(steps = [('norm_price', minmaxscaler_price_xgboost)])\n","\n","column_transformer_xg = ColumnTransformer(transformers = [('price_ma', ma_pipeline_xg, ['Close', 'MA10', 'MA20', 'MA30',\n","                                                                                        'appl_Close', 'msft_Close', 'amzn_Close', \n","                                                                                        'nvda_Close', 'brk_Close', 'googl_Close', \n","                                                                                        'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close']), \n","                                                          ('tfidf', tfidf_pipline_xg, 'tokenized_text'),\n","                                                          ('lm_count', lm_pipeline_xg, 'tokenized_text')])\n","\n","clf_pipeline_binary_xgboost = Pipeline(steps = [('preprocessing', column_transformer_xg), \n","                                                ('classification',clf_xgb)])"]},{"cell_type":"markdown","metadata":{"id":"n7c_xZ8WMLvA"},"source":["### GridSearchCV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEEWp1h-MGFR"},"outputs":[],"source":["parameters_grid_xgb = {'classification__learning_rate': [0.0005, 0.0007, 0.001], \n","                   'classification__max_depth': [3, 4 , 5]}\n","\n","parameters_xgb = ParameterGrid(parameters_grid_xgb)\n","\n","result_xgb = gridsearchcv_xgb(folds_data = dict_folds_data, parametergrid = parameters_xgb, model = clf_pipeline_binary_xgboost)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgmmk0gFMS8Y"},"outputs":[],"source":["parameters_grid_xgb_1 = {'classification__learning_rate': [], 'classification__max_depth': []}\n","\n","df_result_xgb = pd.DataFrame(result_xgb)\n","\n","for parameter in df_result_xgb['parameter']:\n","  for key, value in parameter.items():\n","    parameters_grid_xgb_1[key].append(value)\n","\n","df_result_xgb_1 = df_result_xgb.merge(pd.DataFrame(parameters_grid_xgb_1), how = 'left', left_index = True, right_index = True)\n","df_result_xgb_complete = df_result_xgb_1.iloc[:,1:].groupby(['classification__learning_rate','classification__max_depth'], as_index = False).mean()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDayXOskm2yp"},"outputs":[],"source":["visualize_gridsearchcv_xgb(df_result_xgb_complete)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bd6TDspMmSt"},"outputs":[],"source":["# the best combination of parameters\n","df_result_xgb_complete.iloc[df_result_xgb_complete['valid_f1'].idxmax()]"]},{"cell_type":"markdown","metadata":{"id":"bNMDct4Tch-i"},"source":["### confirm the best model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atfPM0sIchIK"},"outputs":[],"source":["lm_countvectorizer_xgboost = CountVectorizer(vocabulary = lm_sent_dict ['Word'])\n","tfidf_vectorizer_xgboost = TfidfVectorizer(ngram_range = (1, 2), max_features = 10000)\n","minmaxscaler_lm_xgboost = MinMaxScaler()\n","minmaxscaler_price_xgboost = MinMaxScaler()\n","fn_combine_xgboost = FunctionTransformer(combine_count_sent)\n","fn_transform_sparse_xgboost = FunctionTransformer(csr_matrix)\n","clf_xgb = XGBClassifier(booster = 'gbtree', tree_method='gpu_hist', min_split_loss = 0.01, learning_rate = 0.0005, max_depth = 5, n_estimators = 1000, scale_pos_weight = 0.9)\n","\n","tfidf_pipline_xg = Pipeline(steps = [('tfidf_vectorizer', tfidf_vectorizer_xgboost)])\n","\n","lm_pipeline_xg = Pipeline(steps = [('count_vectorizer', lm_countvectorizer_xgboost), \n","                                   ('matrix_mul', fn_combine_xgboost), \n","                                   ('norm', minmaxscaler_lm_xgboost), \n","                                   ('sparse_matrix', fn_transform_sparse_xgboost)])\n","\n","ma_pipeline_xg = Pipeline(steps = [('norm_price', minmaxscaler_price_xgboost)])\n","\n","column_transformer_xg = ColumnTransformer(transformers = [('price_ma', ma_pipeline_xg, ['Close', 'MA10', 'MA20', 'MA30',\n","                                                                                        'appl_Close', 'msft_Close', 'amzn_Close', \n","                                                                                        'nvda_Close', 'brk_Close', 'googl_Close', \n","                                                                                        'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close']), \n","                                                          ('tfidf', tfidf_pipline_xg, 'tokenized_text'),\n","                                                          ('lm_count', lm_pipeline_xg, 'tokenized_text')])\n","\n","clf_pipeline_binary_xgboost = Pipeline(steps = [('preprocessing', column_transformer_xg), \n","                                                ('classification',clf_xgb)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXp2TzMmcx5v"},"outputs":[],"source":["clf_pipeline_binary_xgboost.fit(X_train, y_train['up_down'].values)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDfbPcqevpJE"},"outputs":[],"source":["accuracy_score_valid, f1score_valid = performance_metrics_binary(model = clf_pipeline_binary_xgboost, \n","                                                                 data = X_train, \n","                                                                 true_y = y_train['up_down'].values, \n","                                                                 train_valid_test = 'train')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQEO-gYZxovy"},"outputs":[],"source":["accuracy_score_valid, f1score_valid = performance_metrics_binary(model = clf_pipeline_binary_xgboost, \n","                                                                 data = X_valid, \n","                                                                 true_y = y_valid['up_down'].values, \n","                                                                 train_valid_test = 'valid')"]},{"cell_type":"markdown","metadata":{"id":"jKMwPtp7dSOp"},"source":["## Build Stacking Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G4JGUKXTdMlv"},"outputs":[],"source":["logistic = LogisticRegression()\n","stack_pipeline = StackingClassifier(estimators = [('linearsvc', clf_pipeline_binary), ('xgboost', clf_pipeline_binary_xgboost)], \n","                                    final_estimator = logistic)\n","\n","stack_pipeline.fit(X_train, y_train['up_down'].values)"]},{"cell_type":"markdown","metadata":{"id":"HU2VLQto4NDi"},"source":["### performance evaluation & confusion matrix (training data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7D542WLzIM9"},"outputs":[],"source":["accuracy_score_train, f1score_train = performance_metrics_binary(model = stack_pipeline, \n","                                                                 data = X_train, \n","                                                                 true_y = y_train['up_down'].values, \n","                                                                 train_valid_test = 'train')"]},{"cell_type":"markdown","metadata":{"id":"O-Q8K2l24Ziq"},"source":["### performance evaluation & confusion matrix (valid data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTB7nTPLzYje"},"outputs":[],"source":["accuracy_score_valid, f1score_valid = performance_metrics_binary(model = stack_pipeline, \n","                                                                 data = X_valid, \n","                                                                 true_y = y_valid['up_down'].values, \n","                                                                 train_valid_test = 'valid')"]},{"cell_type":"markdown","metadata":{"id":"127ZXiR31YZY"},"source":["### cross validation (Monte Carlo CV) / 10 folds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hp5GDzQwqxF"},"outputs":[],"source":["folds = ['fold-1', 'fold-2', 'fold-3', 'fold-4', 'fold-5', 'fold-6', 'fold-7', 'fold-8', 'fold-9', 'fold-10']\n","mccv_score_dict = {'fold':[], 'train_accuracy':[], 'train_f1':[], 'valid_accuracy': [], 'valid_f1': []}\n","  \n","for fold in tqdm(folds):\n","  X_train, y_train = dict_folds_data[fold]['train']\n","  X_valid, y_valid = dict_folds_data[fold]['valid']\n","\n","  lm_countvectorizer = CountVectorizer(vocabulary = lm_sent_dict ['Word'])\n","  tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, 2), max_features = 4000)\n","  minmaxscaler = MinMaxScaler()\n","  minmaxscaler_price = MinMaxScaler()\n","  fn_combine = FunctionTransformer(combine_count_sent)\n","  fn_transform_sparse = FunctionTransformer(csr_matrix)\n","\n","  tfidf_pipline = Pipeline(steps = [('tfidf_vectorizer', tfidf_vectorizer)])\n","\n","  lm_pipeline = Pipeline(steps = [('count_vectorizer', lm_countvectorizer), \n","                                  ('matrix_mul', fn_combine), \n","                                  ('norm', minmaxscaler), \n","                                  ('sparse_matrix', fn_transform_sparse)])\n","\n","  ma_pipeline = Pipeline(steps = [('norm_price', minmaxscaler_price)])\n","\n","  column_transformer = ColumnTransformer(transformers = [('price_ma', ma_pipeline, ['Close', 'MA10', 'MA20', 'MA30',\n","                                                                                    'appl_Close', 'msft_Close', 'amzn_Close', \n","                                                                                    'nvda_Close', 'brk_Close', 'googl_Close', \n","                                                                                    'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close']), \n","                                                        ('tfidf', tfidf_pipline, 'tokenized_text'),\n","                                                        ('lm_count', lm_pipeline, 'tokenized_text')])\n","\n","  clf_pipeline_binary = Pipeline(steps = [('preprocessing', column_transformer), \n","                                          ('classification',LinearSVC(C= 0.05, class_weight = 'balanced'))])\n","\n","  #-------------------------------------------------------------------------------------------------------------\n","\n","  lm_countvectorizer_xgboost = CountVectorizer(vocabulary = lm_sent_dict ['Word'])\n","  tfidf_vectorizer_xgboost = TfidfVectorizer(ngram_range = (1, 2), max_features = 10000)\n","  minmaxscaler_lm_xgboost = MinMaxScaler()\n","  minmaxscaler_price_xgboost = MinMaxScaler()\n","  fn_combine_xgboost = FunctionTransformer(combine_count_sent)\n","  fn_transform_sparse_xgboost = FunctionTransformer(csr_matrix)\n","  clf_xgb = XGBClassifier(booster = 'gbtree', tree_method='gpu_hist', min_split_loss = 0.01, learning_rate = 0.0005, max_depth = 5, n_estimators = 1000, scale_pos_weight = 0.9)\n","\n","  tfidf_pipline_xg = Pipeline(steps = [('tfidf_vectorizer', tfidf_vectorizer_xgboost)])\n","\n","  lm_pipeline_xg = Pipeline(steps = [('count_vectorizer', lm_countvectorizer_xgboost), \n","                                    ('matrix_mul', fn_combine_xgboost), \n","                                    ('norm', minmaxscaler_lm_xgboost), \n","                                    ('sparse_matrix', fn_transform_sparse_xgboost)])\n","\n","  ma_pipeline_xg = Pipeline(steps = [('norm_price', minmaxscaler_price_xgboost)])\n","\n","  column_transformer_xg = ColumnTransformer(transformers = [('price_ma', ma_pipeline_xg, ['Close', 'MA10', 'MA20', 'MA30',\n","                                                                                          'appl_Close', 'msft_Close', 'amzn_Close', \n","                                                                                          'nvda_Close', 'brk_Close', 'googl_Close', \n","                                                                                          'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close']), \n","                                                            ('tfidf', tfidf_pipline_xg, 'tokenized_text'),\n","                                                            ('lm_count', lm_pipeline_xg, 'tokenized_text')])\n","\n","  clf_pipeline_binary_xgboost = Pipeline(steps = [('preprocessing', column_transformer_xg), \n","                                                  ('classification',clf_xgb)])\n","  \n","  #--------------------------------------------------------------------------------------------------------------\n","\n","  logistic = LogisticRegression()\n","  stack_pipeline = StackingClassifier(estimators = [('linearsvc', clf_pipeline_binary), ('xgboost', clf_pipeline_binary_xgboost)], \n","                                    final_estimator = logistic)\n","\n","  stack_pipeline.fit(X_train, y_train['up_down'].values)\n","  train_acc = stack_pipeline.score(X_train, y_train['up_down'].values)\n","  predicted_y_train = stack_pipeline.predict(X_train)\n","  train_f1 = f1_score(y_train['up_down'].values, predicted_y_train, average = 'macro')\n","  valid_acc = stack_pipeline.score(X_valid, y_valid['up_down'].values)\n","  predicted_y_valid = stack_pipeline.predict(X_valid)\n","  valid_f1 = f1_score(y_valid['up_down'].values, predicted_y_valid, average = 'macro')\n","\n","  mccv_score_dict['fold'].append(fold)\n","  mccv_score_dict['train_accuracy'].append(train_acc)\n","  mccv_score_dict['train_f1'].append(train_f1)\n","  mccv_score_dict['valid_accuracy'].append(valid_acc)\n","  mccv_score_dict['valid_f1'].append(valid_f1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCeuqNdZnBTC"},"outputs":[],"source":["df_mccv_score_stacking = pd.DataFrame(mccv_score_dict)\n","df_mccv_score_stacking.loc[len(df_mccv_score_stacking)] = ['average', \n","                                                           df_mccv_score_stacking['train_accuracy'].mean(), \n","                                                           df_mccv_score_stacking['train_f1'].mean(), \n","                                                           df_mccv_score_stacking['valid_accuracy'].mean(), \n","                                                           df_mccv_score_stacking['valid_f1'].mean()]\n","\n","df_mccv_score_stacking.loc[len(df_mccv_score_stacking)] = ['std', \n","                                                           df_mccv_score_stacking.iloc[:-1,1].std(), \n","                                                           df_mccv_score_stacking.iloc[:-1,2].std(), \n","                                                           df_mccv_score_stacking.iloc[:-1,3].std(), \n","                                                           df_mccv_score_stacking.iloc[:-1,4].std()]\n","\n","df_mccv_score_stacking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YikKRgVBCzAJ"},"outputs":[],"source":["plt.figure(figsize = (14, 6))\n","ax1 = plt.subplot(2, 2, 1)\n","ax1.plot(df_mccv_score_stacking.iloc[:-2,0], df_mccv_score_stacking.iloc[:-2,1], label = 'train_accuracy', color = 'black', linewidth = 0.5)\n","ax1.axhline(df_mccv_score_stacking.iloc[10, 1], label = 'avg train_accuracy', color = 'black', linewidth = 0.2, linestyle = '--')\n","ax1.set_ylim((0.3, 0.6))\n","ax1.tick_params(labelbottom=False)\n","ax1.legend()\n","ax2 = plt.subplot(2, 2, 3)\n","ax2.plot(df_mccv_score_stacking.iloc[:-2,0], df_mccv_score_stacking.iloc[:-2,3], label = 'valid_accuracy', color = 'black', linewidth = 0.5)\n","ax2.axhline(df_mccv_score_stacking.iloc[10, 3], label = 'avg valid_accuracy', color = 'black', linewidth = 0.2, linestyle = '--')\n","ax2.set_ylim((0.3, 0.6))\n","plt.xticks(rotation = 90)\n","plt.legend()\n","ax3 = plt.subplot(2, 2, 2)\n","ax3.plot(df_mccv_score_stacking.iloc[:-2,0], df_mccv_score_stacking.iloc[:-2,2], label = 'train_f1', color = 'red', linewidth = 0.5)\n","ax3.axhline(df_mccv_score_stacking.iloc[10, 2], label = 'avg train_f1', color = 'red', linewidth = 0.2, linestyle = '--')\n","ax3.set_ylim((0.2, 0.6))\n","ax3.tick_params(labelbottom=False)\n","plt.legend()\n","ax4 = plt.subplot(2, 2, 4)\n","ax4.plot(df_mccv_score_stacking.iloc[:-2,0], df_mccv_score_stacking.iloc[:-2,4], label = 'valid_f1', color = 'red', linewidth = 0.5)\n","ax4.axhline(df_mccv_score_stacking.iloc[10, 4], label = 'avg valid_f1', color = 'red', linewidth = 0.2, linestyle = '--')\n","ax4.set_ylim((0.2, 0.6))\n","plt.xticks(rotation = 90)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1B5ljS5Wdvf7"},"source":["## Stacking Models : performace evaluation & visualization (train on training & valid dataset and test on test dataset)"]},{"cell_type":"markdown","metadata":{"id":"l5R5ACAGiJIT"},"source":["### load train/ valid/ test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9rJdjDJ2dvf7"},"outputs":[],"source":["X_all_train, y_all_train = load_dataset(directory = '../data/TF-IDF Models/News Article Text File_Agg Daily', \n","                                        file_name = 'articles_2015_2019_concated_summaries.txt', \n","                                        sp500_fn = load_sp500_data,\n","                                        fn_tokenization_clean = tokenization_clean,\n","                                        nlp_model = nlp_model, \n","                                        training_data = False)\n","\n","\n","X_all_test, y_all_test = load_dataset(directory = '../data/TF-IDF Models/News Article Text File_Agg Daily', \n","                                           file_name = 'articles_2020_concated_summaries.txt', \n","                                           sp500_fn = load_sp500_data,\n","                                           fn_tokenization_clean = tokenization_clean,\n","                                           nlp_model = nlp_model,\n","                                           training_data = False)"]},{"cell_type":"markdown","metadata":{"id":"C_IgP_wWiO7W"},"source":["### initialize & train stacking model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfyuVomCexIi"},"outputs":[],"source":["lm_countvectorizer = CountVectorizer(vocabulary = lm_sent_dict ['Word'])\n","tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, 2), max_features = 4000)\n","minmaxscaler = MinMaxScaler()\n","minmaxscaler_price = MinMaxScaler()\n","fn_combine = FunctionTransformer(combine_count_sent)\n","fn_transform_sparse = FunctionTransformer(csr_matrix)\n","\n","tfidf_pipline = Pipeline(steps = [('tfidf_vectorizer', tfidf_vectorizer)])\n","\n","lm_pipeline = Pipeline(steps = [('count_vectorizer', lm_countvectorizer), \n","                                ('matrix_mul', fn_combine), \n","                                ('norm', minmaxscaler), \n","                                ('sparse_matrix', fn_transform_sparse)])\n","\n","ma_pipeline = Pipeline(steps = [('norm_price', minmaxscaler_price)])\n","\n","column_transformer = ColumnTransformer(transformers = [('price_ma', ma_pipeline, ['Close', 'MA10', 'MA20', 'MA30',\n","                                                                                  'appl_Close', 'msft_Close', 'amzn_Close', \n","                                                                                  'nvda_Close', 'brk_Close', 'googl_Close', \n","                                                                                  'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close']), \n","                                                      ('tfidf', tfidf_pipline, 'tokenized_text'),\n","                                                      ('lm_count', lm_pipeline, 'tokenized_text')])\n","\n","clf_pipeline_binary = Pipeline(steps = [('preprocessing', column_transformer), \n","                                        ('classification',LinearSVC(C= 0.05, class_weight = 'balanced'))])\n","\n","#-------------------------------------------------------------------------------------------------------------\n","\n","lm_countvectorizer_xgboost = CountVectorizer(vocabulary = lm_sent_dict ['Word'])\n","tfidf_vectorizer_xgboost = TfidfVectorizer(ngram_range = (1, 2), max_features = 10000)\n","minmaxscaler_lm_xgboost = MinMaxScaler()\n","minmaxscaler_price_xgboost = MinMaxScaler()\n","fn_combine_xgboost = FunctionTransformer(combine_count_sent)\n","fn_transform_sparse_xgboost = FunctionTransformer(csr_matrix)\n","clf_xgb = XGBClassifier(booster = 'gbtree', tree_method='gpu_hist', min_split_loss = 0.01, learning_rate = 0.0005, max_depth = 5, n_estimators = 1000, scale_pos_weight = 0.9)\n","\n","tfidf_pipline_xg = Pipeline(steps = [('tfidf_vectorizer', tfidf_vectorizer_xgboost)])\n","\n","lm_pipeline_xg = Pipeline(steps = [('count_vectorizer', lm_countvectorizer_xgboost), \n","                                  ('matrix_mul', fn_combine_xgboost), \n","                                  ('norm', minmaxscaler_lm_xgboost), \n","                                  ('sparse_matrix', fn_transform_sparse_xgboost)])\n","\n","ma_pipeline_xg = Pipeline(steps = [('norm_price', minmaxscaler_price_xgboost)])\n","\n","column_transformer_xg = ColumnTransformer(transformers = [('price_ma', ma_pipeline_xg, ['Close', 'MA10', 'MA20', 'MA30',\n","                                                                                        'appl_Close', 'msft_Close', 'amzn_Close', \n","                                                                                        'nvda_Close', 'brk_Close', 'googl_Close', \n","                                                                                        'tsla_Cloae', 'meta_Close', 'xom_Close', 'jpm_Close']), \n","                                                          ('tfidf', tfidf_pipline_xg, 'tokenized_text'),\n","                                                          ('lm_count', lm_pipeline_xg, 'tokenized_text')])\n","\n","clf_pipeline_binary_xgboost = Pipeline(steps = [('preprocessing', column_transformer_xg), \n","                                                ('classification',clf_xgb)])\n","\n","#--------------------------------------------------------------------------------------------------------------\n","\n","logistic = LogisticRegression()\n","stack_pipeline = StackingClassifier(estimators = [('linearsvc', clf_pipeline_binary), ('xgboost', clf_pipeline_binary_xgboost)], \n","                                  final_estimator = logistic)\n","\n","stack_pipeline.fit(X_all_train, y_all_train['up_down'].values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1jd223ywmSma"},"outputs":[],"source":["accuracy_score_all_train, f1score_all_train = performance_metrics_binary(model = stack_pipeline,\n","                                                                         data = X_all_train,\n","                                                                         true_y = y_all_train['up_down'].values,\n","                                                                         train_valid_test = 'train_valid')"]},{"cell_type":"markdown","metadata":{"id":"FY-p-P8LifyV"},"source":["### performance evaluation & confusion matrix (test dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDhU7gzSiYWx"},"outputs":[],"source":["accuracy_score_all, f1score_all = performance_metrics_binary(model = stack_pipeline,\n","                                                             data = X_all_test,\n","                                                             true_y = y_all_test['up_down'].values, \n","                                                             train_valid_test = 'test')"]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1CnO1VK3VnozMJX49bSoq4vchRzr2oyss","authorship_tag":"ABX9TyOyOQ7d1DJWGTIUlkgh928M"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}