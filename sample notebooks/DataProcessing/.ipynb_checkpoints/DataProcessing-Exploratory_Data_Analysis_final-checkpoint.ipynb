{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg9t9G0zvPWQ"
   },
   "source": [
    "## Install/ Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XM5gVVhNxbvW"
   },
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "! pip install -U spacy\n",
    "! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IthhjjDgvDrZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import yfinance\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5cF2DZQvlhT"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-JPC0m2vpjw"
   },
   "source": [
    "### load news data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LI9p5VmUvdJ4"
   },
   "outputs": [],
   "source": [
    "def load_news_data(directory_files: 'str'):\n",
    "  directory_path = Path(directory_files)\n",
    "  directory_files = os.listdir(directory_path)\n",
    "\n",
    "  file_dict = {}\n",
    "  for file_name in directory_files:\n",
    "    if file_name.split('.')[-1] == 'csv':\n",
    "      try:\n",
    "        df = pd.read_csv(directory_path / file_name)\n",
    "        file_dict[file_name] = df\n",
    "      except:\n",
    "        pass\n",
    "    elif file_name.split('.')[-1] == 'json':\n",
    "      try:\n",
    "        df = pd.read_json(directory_path / file_name)\n",
    "        file_dict[file_name] = df\n",
    "      except:\n",
    "        try:\n",
    "          df = pd.read_json(directory_path / file_name, lines = True)\n",
    "          file_dict[file_name] = df\n",
    "        except:\n",
    "          pass\n",
    "    elif  file_name.split('.')[-1] == 'txt':\n",
    "      try:\n",
    "        with open(directory_path / file_name, 'r') as f:\n",
    "          data = eval(f.read())\n",
    "          df = pd.DataFrame(data).T\n",
    "          file_dict[file_name] = df\n",
    "      except:\n",
    "        pass\n",
    "  return file_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3V3A_TqxJfu"
   },
   "source": [
    "### load sp500 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cgr6qJMwCR2"
   },
   "outputs": [],
   "source": [
    "def load_sp500_data(start_date = '2014-12-31', end_date = '2021-01-05'):\n",
    "  sp = yfinance.Ticker('^GSPC')\n",
    "  sp_history = sp.history(start =start_date, end = end_date)\n",
    "  sp_history.reset_index(inplace = True)\n",
    "  sp_history['Date'] = pd.to_datetime(sp_history['Date'].dt.date)\n",
    "  sp_history.drop(labels = ['Dividends', 'Stock Splits'], axis = 1, inplace = True)\n",
    "  sp_history = sp_history.loc[:,['Date', 'Open', 'High', 'Low', 'Volume', 'Close']]\n",
    "  sp_history['Close+1day'] = sp_history['Close'].shift(-1)\n",
    "  sp_history.dropna(inplace = True)\n",
    "  sp_history['up_down'] = sp_history[['Close', 'Close+1day']].apply(lambda x: 1 if x['Close+1day'] > x['Close'] else 0, axis = 1)\n",
    "  return sp_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zs2H0CYkDBim"
   },
   "source": [
    "### tokenize & clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zPh5OY1DGAn"
   },
   "outputs": [],
   "source": [
    "def tokenization_clean(text: 'str', nlp_model: 'spacy_model'):\n",
    "  doc = nlp_model(text)\n",
    "  tok_aft_spacy = [re.sub(r'[^\\w\\s]', '', tok.lemma_.lower()) for tok in doc \n",
    "                   if not tok.is_stop\n",
    "                   and not tok.is_punct \n",
    "                   and not tok.like_num \n",
    "                   and not tok.like_url \n",
    "                   and not tok.is_space \n",
    "                   and not tok.like_email \n",
    "                   and not tok.is_left_punct \n",
    "                   and not tok.is_right_punct \n",
    "                   and not tok.is_digit \n",
    "                   and not tok.is_currency]\n",
    "  \n",
    "  join_tok_aft_spacy = ' '.join(tok_aft_spacy)\n",
    "  return join_tok_aft_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QehI0B7pCV7X"
   },
   "source": [
    "### distribution of word counts in each news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVOdU00jCe3J"
   },
   "outputs": [],
   "source": [
    "def distribution_word_count(load_news_data_fn: 'function', directory, file_name, tokenization_clean_fn: 'function', nlp_model):\n",
    "  tqdm.pandas()\n",
    "  data_dict = load_news_data(directory_files = directory)\n",
    "  news_data = data_dict[file_name]\n",
    "  news_data['timestamp'] = pd.to_datetime(news_data['timestamp'])\n",
    "  news_data['tokenized_text'] = news_data['text'].progress_apply(lambda x: tokenization_clean_fn(x, nlp_model))\n",
    "  news_data['word_count_text'] = news_data['text'].progress_apply(lambda x: len(x.split(' ')))\n",
    "  news_data['word_count_tokenized'] = news_data['tokenized_text'].progress_apply(lambda x: len(x.split(' ')))\n",
    "  news_data['bin_count_text'] = pd.cut(news_data['word_count_text'], [0, 20, 40, 60, 80, 100, 120, 140, 180, 200, \n",
    "                                                                      220, 240, 260, 280, 300, 320, 340, 360, 380, 400])\n",
    "  news_data['bin_count_tokenized'] = pd.cut(news_data['word_count_tokenized'], [0, 20, 40, 60, 80, 100, 120, 140, 180, 200, \n",
    "                                                                                220, 240, 260, 280, 300, 320, 340, 360, 380, 400])\n",
    "  news_data_groupby_1 = news_data[['bin_count_text', 'text']].groupby('bin_count_text', as_index = False).count()\n",
    "  news_data_groupby_1.rename(columns = {'text': 'text_count'}, inplace = True)\n",
    "  news_data_groupby_2 = news_data[['bin_count_tokenized', 'text']].groupby('bin_count_tokenized', as_index = False).count()\n",
    "  news_data_groupby_2.rename(columns = {'text': 'processed text_count'}, inplace = True)\n",
    "  news_data_groupby = news_data_groupby_1.merge(news_data_groupby_2, how = 'outer', left_on = 'bin_count_text', right_on = 'bin_count_tokenized')\n",
    "  \n",
    "  plt.figure(figsize = (10, 5))\n",
    "  plt.plot(news_data_groupby['bin_count_text'].astype(str), news_data_groupby['text_count'], linewidth = 0.3, color = 'black', label = 'word count before processing')\n",
    "  plt.plot(news_data_groupby['bin_count_text'].astype(str), news_data_groupby['processed text_count'], linewidth = 0.3, color = 'red', label = 'word count after processing')\n",
    "  plt.xlabel('bin of word count')\n",
    "  plt.xticks(rotation = 90)\n",
    "  plt.ylabel('number of news article')\n",
    "  plt.title('distribution of words in each article')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  return print(f\"min word count before processing: {news_data['word_count_text'].min()}, min word count after processing: {news_data['word_count_tokenized'].min()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OtTw2QrQ8EW"
   },
   "source": [
    "### distribution of news articles by date (line chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPH6xp7aRB-d"
   },
   "outputs": [],
   "source": [
    "def distribution_news_date(load_news_data_fn: 'function', directory, file_name):\n",
    "  data_dict = load_news_data(directory_files = directory)\n",
    "  news_data = data_dict[file_name]\n",
    "  news_data['timestamp'] = pd.to_datetime(news_data['timestamp'])\n",
    "  news_count_groupby = news_data[['text','timestamp']].groupby('timestamp').count()\n",
    "  news_count_groupby = news_count_groupby.rename(columns = {'text': 'daily count of news'})\n",
    "  news_count_groupby.plot(figsize = (16, 6), color = 'black', linewidth = 0.2, label = 'daily count of news')\n",
    "  plt.ylabel('count of news')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  return news_count_groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4xe2EiI-eUx"
   },
   "source": [
    "### distribution of news sentiment (bar chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUnNOINM-yh2"
   },
   "outputs": [],
   "source": [
    "def distribution_news_sent(load_news_data_fn: 'function', directory, file_name):\n",
    "  data_dict = load_news_data(directory_files = directory)\n",
    "  news_data = data_dict[file_name]\n",
    "  news_data['timestamp'] = pd.to_datetime(news_data['timestamp'])\n",
    "  sent_groupby = news_data[['text','sentiment']].groupby('sentiment').count()\n",
    "  sent_groupby.rename(columns = {'text': 'count of sentiment'}, inplace = True)\n",
    "\n",
    "  plt.figure(figsize = (6, 6))\n",
    "  barchart = plt.bar(sent_groupby.index, sent_groupby['count of sentiment'], width=0.5)\n",
    "  barchart[0].set_color('#AED6F1')\n",
    "  barchart[1].set_color('#2E86C1')\n",
    "\n",
    "  for idx, (i, j) in enumerate(sent_groupby.to_dict()['count of sentiment'].items()):\n",
    "    plt.annotate('{:0,}'.format(j), xy = (idx - 0.05, j + 1000))\n",
    "  \n",
    "  plt.ylim(0, 50000)\n",
    "  plt.title('Distribution of News Sentiment')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwfn79IOTSxy"
   },
   "source": [
    "### distribution of news topics (bar chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3I9z1MkQm8Z"
   },
   "outputs": [],
   "source": [
    "def distribution_news_topic(load_news_data_fn: 'function', directory, file_name):\n",
    "  data_dict = load_news_data(directory_files = directory)\n",
    "  news_data = data_dict[file_name]\n",
    "  news_data['timestamp'] = pd.to_datetime(news_data['timestamp'])\n",
    "  topic_groupby = news_data[['text','topics']].groupby('topics').count()\n",
    "  topic_groupby.rename(columns = {'text': 'count of topics'}, inplace = True)\n",
    "\n",
    "  plt.figure(figsize = (12, 6))\n",
    "  barchart = plt.bar(topic_groupby.index, topic_groupby['count of topics'], width=0.7)\n",
    "  \n",
    "  for idx, color in zip(range(4), ['#D6EAF8', '#85C1E9', '#3498DB', '#21618C']):\n",
    "    barchart[idx].set_color(color)\n",
    "\n",
    "  for idx, (i, j) in enumerate(topic_groupby.to_dict()['count of topics'].items()):\n",
    "    plt.annotate('{:0,}'.format(j), xy = (idx - 0.08, j + 1000))\n",
    "  \n",
    "  plt.ylim(0, 50000)\n",
    "  plt.title('Distribution of News Topics')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkTbkxP4V-EL"
   },
   "source": [
    "### trend of daliy overall sentiment  & sp500 close price (line chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBWEl9beV9uE"
   },
   "outputs": [],
   "source": [
    "def distribution_overall_sent(load_news_data_fn: 'function', directory, file_name, sp500_fn: 'function'):\n",
    "  data_dict = load_news_data(directory_files = directory)\n",
    "  news_data = data_dict[file_name]\n",
    "  news_data['timestamp'] = pd.to_datetime(news_data['timestamp'])\n",
    "\n",
    "  sp500 = sp500_fn()\n",
    "  sp500 = sp500[sp500['Date'] <= '2019-12-31'].copy()\n",
    "\n",
    "  df_all_groupby = news_data[['timestamp', 'text', 'sentiment']].groupby(['timestamp', 'sentiment'], as_index = False).count()\n",
    "  df_all_groupby_norm = pd.pivot(df_all_groupby, index= 'timestamp', columns= 'sentiment', values= 'text').fillna(0).reset_index()\n",
    "  df_all_groupby_norm['negative_norm'] = (df_all_groupby_norm['negative'] - df_all_groupby_norm['negative'].min())/df_all_groupby_norm['negative'].max()\n",
    "  df_all_groupby_norm['positive_norm'] = (df_all_groupby_norm['positive'] - df_all_groupby_norm['positive'].min())/df_all_groupby_norm['positive'].max()\n",
    "\n",
    "  df_all_groupby_norm['negative_weight'] = df_all_groupby_norm['negative']/np.sum(df_all_groupby_norm[['negative', 'positive']], axis = 1)\n",
    "  df_all_groupby_norm['positive_weight'] = df_all_groupby_norm['positive']/np.sum(df_all_groupby_norm[['negative', 'positive']], axis = 1)\n",
    "\n",
    "  df_all_groupby_norm['overall_sentiment'] = (df_all_groupby_norm['positive_weight'] * df_all_groupby_norm['positive_norm']*1) \\\n",
    "  - (df_all_groupby_norm['negative_weight'] * df_all_groupby_norm['negative_norm']*1)\n",
    "\n",
    "  df_all_groupby_norm['overall_sentiment_ma'] = df_all_groupby_norm['overall_sentiment'].rolling(90).mean()\n",
    "\n",
    "  fig,ax = plt.subplots(figsize = (12, 6))\n",
    "  ax2=ax.twinx()\n",
    "\n",
    "  ax.plot(df_all_groupby_norm['timestamp'], df_all_groupby_norm['overall_sentiment_ma'], \n",
    "         label = 'overall_sentiment (moving average)', color = 'black', linewidth = 0.2)\n",
    "  ax.legend()\n",
    "  ax.set_ylabel('overall sentiment (moving average)')\n",
    "  \n",
    "  ax2.plot(sp500['Date'], sp500['Close'], label = 'SP500 Close Price', color = 'red', linewidth = 0.2)\n",
    "  ax2.legend()\n",
    "  ax2.set_ylabel('sp500 close price')\n",
    "\n",
    "  plt.xlabel('date')\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_9bfkdS9df5"
   },
   "source": [
    "## EDA: News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKtY6rOqENWV"
   },
   "source": [
    "### initialize spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MztKNAN1ERNr"
   },
   "outputs": [],
   "source": [
    "nlp_model = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw7nACMoCP3l"
   },
   "source": [
    "### distribution of word counts in each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUkcjFXUIX-U"
   },
   "outputs": [],
   "source": [
    "distribution_word_count(load_news_data_fn = load_news_data, \n",
    "                        directory = '../data/EDA/News Article Text File', \n",
    "                        file_name = 'articles_2015_2019.csv', \n",
    "                        tokenization_clean_fn = tokenization_clean, \n",
    "                        nlp_model = nlp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghb8UhPqQgB2"
   },
   "source": [
    "### distribution of daily news article count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtE64fG8SSYA"
   },
   "outputs": [],
   "source": [
    "output = distribution_news_date(load_news_data_fn = load_sp500_data, \n",
    "                       directory = '../data/EDA/News Article Text File',\n",
    "                       file_name = 'articles_2015_2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWn42EsbYQRN"
   },
   "source": [
    "### distribution of news sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QomFgZG790TS"
   },
   "outputs": [],
   "source": [
    "distribution_news_sent(load_news_data_fn = load_sp500_data, \n",
    "                       directory = '../data/EDA/News Article Text File',\n",
    "                       file_name = 'articles_2015_2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1eRPfVkYUf4"
   },
   "source": [
    "### distribution of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovCP2kAK-FdR"
   },
   "outputs": [],
   "source": [
    "distribution_news_topic(load_news_data_fn = load_sp500_data, \n",
    "                        directory = '../data/EDA/News Article Text File',\n",
    "                        file_name = 'articles_2015_2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_NCfv9yYaU1"
   },
   "source": [
    "### trend of overall sentiment & sp500 close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHb3HduqXoIA"
   },
   "outputs": [],
   "source": [
    "distribution_overall_sent(load_news_data_fn = load_sp500_data, \n",
    "                          directory = '../data/EDA/News Article Text File',\n",
    "                          file_name = 'articles_2015_2019.csv', \n",
    "                          sp500_fn = load_sp500_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMVCnOl6ruqifV5dOVb2c0b",
   "mount_file_id": "1cV3c0zZgzMuzzpza_CW3u-ltgkN6Dx_i",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
