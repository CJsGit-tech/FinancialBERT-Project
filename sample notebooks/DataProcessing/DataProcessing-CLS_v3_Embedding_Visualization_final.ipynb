{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro9hpuKNjzqj"
   },
   "source": [
    "## Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeMNxinVvPb3"
   },
   "outputs": [],
   "source": [
    "# Installation\n",
    "! pip install bert-extractive-summarizer\n",
    "! pip install sentencepiece\n",
    "! pip install datasets\n",
    "! pip install -U spacy\n",
    "! python -m spacy download en_core_web_lg\n",
    "! pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXN28I65kVzJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from transformers import *\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from summarizer import Summarizer\n",
    "import umap\n",
    "import spacy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5bOfaE4m5bk"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3KXPqPqlMYa"
   },
   "source": [
    "### summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6X9aSfxkgbB"
   },
   "outputs": [],
   "source": [
    "def get_summarizer(model_name):\n",
    "  # Find and setup model from the HuggingFace API\n",
    "    custom_config = AutoConfig.from_pretrained(model_name)\n",
    "    custom_config.output_hidden_states=True\n",
    "    custom_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    custom_model = AutoModel.from_pretrained(model_name, config=custom_config)\n",
    "\n",
    "  # Initiate the bert-extractive summarizar\n",
    "    model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
    "    return model\n",
    "\n",
    "def perform_summarizer(text_body, model, num_sent = 5, return_embeddings = False):\n",
    "    try:\n",
    "      text_body = text_body.strip()\n",
    "      text_body = text_body.replace('\\n',' ')\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    # Return Summary\n",
    "    summary = model(body = text_body, num_sentences = num_sent, max_length = 800)\n",
    "\n",
    "    if return_embeddings:\n",
    "        embeddings = model.run_embeddings(summary, aggregate = 'mean', num_sentences = num_sent)\n",
    "        return summary, embeddings\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY_jRS4C2kKQ"
   },
   "source": [
    "### sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrLQ5F6n2m66"
   },
   "outputs": [],
   "source": [
    "def get_sentiment_model(model):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model, model_max_length=512)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "  return tokenizer, model\n",
    "\n",
    "def get_sentiment_prediction(tokenizer,model, text):\n",
    "  try:\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.strip()\n",
    "  except:\n",
    "    pass\n",
    "  tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "  classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, **tokenizer_kwargs)\n",
    "  prediction = classifier(str(text))[0]\n",
    "  label, score = prediction['label'], prediction['score']\n",
    "  return label, round(score,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIeZS1qwKkn9"
   },
   "source": [
    "### topic analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIup9JXFKn9J"
   },
   "outputs": [],
   "source": [
    "def get_topic_model(model):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model,from_tf=True)\n",
    "  return tokenizer, model\n",
    "\n",
    "def get_topic_prediction(tokenizer,model, text):\n",
    "  try:\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.strip()\n",
    "  except:\n",
    "    pass\n",
    "  tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512}\n",
    "  classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, **tokenizer_kwargs)\n",
    "  prediction = classifier(str(text))[0]\n",
    "  label, score = prediction['label'], prediction['score']\n",
    "  return label, round(score,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X22g23spu19e"
   },
   "source": [
    "### load news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AperN7JTu048"
   },
   "outputs": [],
   "source": [
    "def load_news_data(directory_files: 'str'):\n",
    "  directory_path = Path(directory_files)\n",
    "  directory_files = os.listdir(directory_path)\n",
    "\n",
    "  file_dict = {}\n",
    "  for file_name in directory_files:\n",
    "    if file_name.split('.')[-1] == 'csv':\n",
    "      try:\n",
    "        df = pd.read_csv(directory_path / file_name)\n",
    "        file_dict[file_name] = df\n",
    "      except:\n",
    "        pass\n",
    "    elif file_name.split('.')[-1] == 'json':\n",
    "      try:\n",
    "        df = pd.read_json(directory_path / file_name)\n",
    "        file_dict[file_name] = df\n",
    "      except:\n",
    "        try:\n",
    "          df = pd.read_json(directory_path / file_name, lines = True)\n",
    "          file_dict[file_name] = df\n",
    "        except:\n",
    "          pass\n",
    "    elif  file_name.split('.')[-1] == 'txt':\n",
    "      try:\n",
    "        with open(directory_path / file_name, 'r') as f:\n",
    "          data = eval(f.read())\n",
    "          df = pd.DataFrame(data).T\n",
    "          file_dict[file_name] = df\n",
    "      except:\n",
    "        pass\n",
    "  return file_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrwlOpTqu80f"
   },
   "source": [
    "### generate embedding for sampled news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlpfoiHYvD2s"
   },
   "outputs": [],
   "source": [
    "def tokenization_embedding(load_news_data_fn: 'function', \n",
    "                           directory: 'str', \n",
    "                           file_name: 'str', \n",
    "                           perform_summarizer: 'function', \n",
    "                           summarizer_model: 'function', \n",
    "                           num_sent = 2, \n",
    "                           return_embeddings = True, \n",
    "                           sample_size = 300):\n",
    "  \n",
    "  warnings.filterwarnings(\"ignore\")\n",
    "  tqdm.pandas()\n",
    "  data_dict = load_news_data(directory_files = directory)\n",
    "  news_data = data_dict[file_name]\n",
    "  news_data['token_count'] = news_data['text'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "  news_data_biz = news_data[(news_data['topics'] == 'Business & Finance') & (news_data['token_count'] <=100)].sample(sample_size)\n",
    "  news_data_pol = news_data[(news_data['topics'] == 'Politics & Government') & (news_data['token_count'] <=100)].sample(sample_size)\n",
    "  news_data_sci = news_data[(news_data['topics'] == 'Science & Mathematics') & (news_data['token_count'] <=100)].sample(sample_size)\n",
    "  news_data_com = news_data[(news_data['topics'] == 'Computers & Internet') & (news_data['token_count'] <=100)].sample(sample_size)\n",
    "\n",
    "  news_data_sampled = pd.concat([news_data_biz, news_data_pol, news_data_sci, news_data_com], ignore_index=True)\n",
    "  news_data_sampled['summary'], news_data_sampled['text_embedding'] = zip(*news_data_sampled['text'].progress_apply(\n",
    "      lambda x: perform_summarizer(text_body = x, model = summarizer_model, num_sent = num_sent, return_embeddings = return_embeddings))\n",
    "  )\n",
    "\n",
    "  text_article_embedding = [i for i in news_data_sampled['text_embedding']]\n",
    "  df_text_article_embedding = pd.DataFrame(np.vstack(text_article_embedding))\n",
    "  df_text_article_embedding['topics'] = news_data_sampled['topics'].values\n",
    "  df_text_article_embedding['sentiment'] = news_data_sampled['sentiment'].values\n",
    "\n",
    "  return news_data_sampled, df_text_article_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PER6fM_n1Was"
   },
   "source": [
    "### get the logit of BERT sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aavNOPc71nqj"
   },
   "outputs": [],
   "source": [
    "def logit_sent(news_data: 'df', financial_tokenizer: 'function', financial_model: 'function', summary: 'True/False'):\n",
    "  logit_list = []\n",
    "\n",
    "  if summary == False:\n",
    "    for text in tqdm(news_data['text']):\n",
    "        input = torch.tensor(financial_tokenizer(text, truncation = True, max_length = 512, padding = True,).input_ids).unsqueeze(0)\n",
    "        tensor_output = financial_model(input)[0].detach()\n",
    "        logit_list.append(tensor_output)\n",
    "  \n",
    "  elif summary == True:\n",
    "    for text in tqdm(news_data['summary']):\n",
    "        input = torch.tensor(financial_tokenizer(text, truncation = True, max_length = 512, padding = True,).input_ids).unsqueeze(0)\n",
    "        tensor_output = financial_model(input)[0].detach()\n",
    "        logit_list.append(tensor_output)\n",
    "\n",
    "  logit_output_all = torch.stack(logit_list)\n",
    "  df_logit_output = pd.DataFrame(logit_output_all.squeeze().numpy())\n",
    "  df_logit_output['sentiment'] = news_data['sentiment']\n",
    "\n",
    "  return df_logit_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_TRJNR2CnEd"
   },
   "source": [
    "### visualize sentiment logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCEohRlICrzv"
   },
   "outputs": [],
   "source": [
    "def logit_visualize(df_logit: 'DataFrame', X_transformed: 'array'):\n",
    "  np.random.seed(42)\n",
    "  df_tsne = pd.DataFrame(X_transformed, columns = ['d1', 'd2'])\n",
    "  df_tsne['sentiment'] = df_logit['sentiment'].values\n",
    "\n",
    "  plt.figure(figsize = (7, 7))\n",
    "  color = ['#EC7063', '#2980B9']\n",
    "\n",
    "  for i, j in zip(['positive', 'negative'], color):\n",
    "    plt.scatter(df_tsne.loc[(df_tsne['sentiment'] == i),'d1'], \n",
    "                df_tsne.loc[(df_tsne['sentiment'] == i),'d2'],   \n",
    "                c = j, label = i, s = 6)\n",
    "\n",
    "  plt.legend()\n",
    "  plt.xlabel('dimension 1')\n",
    "  plt.ylabel('dimension 2')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UiQhq4_J7lN"
   },
   "source": [
    "### get the logit of BERT topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad_9hmnGKAtY"
   },
   "outputs": [],
   "source": [
    "def logit_topic(news_data: 'df', topic_tokenizer: 'function', topic_model: 'function', summary: 'True/False'):\n",
    "  logit_list = []\n",
    "\n",
    "  if summary == False:\n",
    "    for text in tqdm(news_data['text']):\n",
    "        input = torch.tensor(topic_tokenizer(text, truncation = True, max_length = 512, padding = True,).input_ids).unsqueeze(0)\n",
    "        tensor_output = topic_model(input)[0].detach()\n",
    "        logit_list.append(tensor_output)\n",
    "  \n",
    "  elif summary == True:\n",
    "    for text in tqdm(news_data['summary']):\n",
    "        input = torch.tensor(topic_tokenizer(text, truncation = True, max_length = 512, padding = True,).input_ids).unsqueeze(0)\n",
    "        tensor_output = topic_model(input)[0].detach()\n",
    "        logit_list.append(tensor_output)\n",
    "  \n",
    "  logit_output_all = torch.stack(logit_list)\n",
    "  df_logit_output = pd.DataFrame(logit_output_all.squeeze().numpy())\n",
    "  df_logit_output['topics'] = news_data['topics']\n",
    "\n",
    "  return df_logit_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceofx-LXLkq-"
   },
   "source": [
    "### visualize topic logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVHSLa2fLrxi"
   },
   "outputs": [],
   "source": [
    "def logit_topic_visualize(df_logit: 'DataFrame', X_transformed: 'array'):\n",
    "  np.random.seed(42)\n",
    "  df_tsne = pd.DataFrame(X_transformed, columns = ['d1', 'd2'])\n",
    "  df_tsne['topics'] = df_logit['topics'].values\n",
    "\n",
    "  plt.figure(figsize = (7, 7))\n",
    "  color = ['#2980B9', '#27AE60', '#F1C40F', '#7F8C8D']\n",
    "  topic_list = ['Business & Finance', 'Politics & Government','Science & Mathematics', 'Computers & Internet']\n",
    "\n",
    "  for i, j in zip(topic_list, color):\n",
    "    plt.scatter(df_tsne.loc[(df_tsne['topics'] == i),'d1'], \n",
    "                df_tsne.loc[(df_tsne['topics'] == i),'d2'],   \n",
    "                c = j, label = i, s = 6)\n",
    "\n",
    "  plt.legend()\n",
    "  plt.xlabel('dimension 1')\n",
    "  plt.ylabel('dimension 2')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KyZIQnb5rlA"
   },
   "source": [
    "## Visualization of Summary Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp_Pd9KEnA2A"
   },
   "source": [
    "### initialize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PG0u0XNlH_q"
   },
   "outputs": [],
   "source": [
    "model_name = 'facebook/bart-large-cnn'\n",
    "summarizer_model = get_summarizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SUECcS_vQbT"
   },
   "outputs": [],
   "source": [
    "news_data_sampled, df_embedding = tokenization_embedding(load_news_data_fn = load_news_data, \n",
    "                                                         directory = '../data/EDA/News Article Text File', \n",
    "                                                         file_name = 'articles_2015_2019.csv', \n",
    "                                                         perform_summarizer = perform_summarizer,\n",
    "                                                         summarizer_model = summarizer_model, \n",
    "                                                         num_sent = 2, \n",
    "                                                         return_embeddings = True, \n",
    "                                                         sample_size = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bEtOUhrE502"
   },
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAQkwfNuD39n"
   },
   "source": [
    "#### reduce the dimensionality of summary embedding to 3 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OvFntioEzhP"
   },
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(n_neighbors=30,\n",
    "                       n_components=3,\n",
    "                       metric='cosine', \n",
    "                       random_state = 42)\n",
    "\n",
    "X_transformed_umap = umap_model.fit_transform(df_embedding.iloc[:,:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRR3uS4NEaQ6"
   },
   "source": [
    "#### visualize transformed embedding/ label = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkQBJSzyFvvm"
   },
   "outputs": [],
   "source": [
    "df_umap = pd.DataFrame(X_transformed_umap, columns = ['d1', 'd2', 'd3'])\n",
    "df_umap['topic_label'] = df_embedding['topics'].values\n",
    "\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "unique_topic = set(df_embedding['topics'].values)\n",
    "color = ['#2980B9', '#27AE60', '#F1C40F', '#7F8C8D']\n",
    "\n",
    "\n",
    "\n",
    "for i, j in zip(unique_topic, color):\n",
    "  ax.scatter(df_umap.loc[(df_umap['topic_label'] == i),'d1'], \n",
    "             df_umap.loc[(df_umap['topic_label'] == i),'d2'],\n",
    "             df_umap.loc[(df_umap['topic_label'] == i),'d3'],\n",
    "              c = j, label = i)\n",
    "\n",
    "ax.set_xlabel('dimension 1')\n",
    "ax.set_ylabel('dimension 2')\n",
    "ax.set_zlabel('dimension 3')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6L5tRHZEPsu"
   },
   "source": [
    "#### visualize transformed embedding/ label = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeQGkmWdHw9r"
   },
   "outputs": [],
   "source": [
    "df_umap = pd.DataFrame(X_transformed_umap, columns = ['d1', 'd2', 'd3'])\n",
    "df_umap['sentiment_label'] = df_embedding['sentiment'].values\n",
    "\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "unique_topic = set(df_embedding['sentiment'].values)\n",
    "color = ['#2980B9', '#27AE60']\n",
    "\n",
    "\n",
    "\n",
    "for i, j in zip(unique_topic, color): \n",
    "  ax.scatter(df_umap.loc[(df_umap['sentiment_label'] == i),'d1'], \n",
    "             df_umap.loc[(df_umap['sentiment_label'] == i),'d2'],\n",
    "             df_umap.loc[(df_umap['sentiment_label'] == i),'d3'],\n",
    "              c = j, label = i)\n",
    "\n",
    "ax.set_xlabel('dimension 1')\n",
    "ax.set_ylabel('dimension 2')\n",
    "ax.set_zlabel('dimension 3')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BLc4XPtE0Pt"
   },
   "source": [
    "### tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3GBUdNfQccW"
   },
   "outputs": [],
   "source": [
    "parameter_dict = {'learning_rate': [50, 100, 150, 200],\n",
    "                  'perplexity': [10, 30, 50], \n",
    "                  'early_exaggeration': [20]}\n",
    "grid = ParameterGrid(parameter_dict)\n",
    "\n",
    "KL_list = []\n",
    "complete = 0\n",
    "for i in grid:\n",
    "  TSNE_model = TSNE(n_components=2, \n",
    "                    learning_rate=i['learning_rate'], \n",
    "                    init='random', \n",
    "                    perplexity=i['perplexity'],\n",
    "                    method = 'exact',\n",
    "                    early_exaggeration =i['early_exaggeration']).fit(df_embedding.iloc[:,:-2])\n",
    "  KL_list.append(TSNE_model.kl_divergence_)\n",
    "  complete += 1\n",
    "  print(f'finish: {complete}/ {len(grid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Mi6xTvPRWMg"
   },
   "outputs": [],
   "source": [
    "best_parameter = grid[np.array(KL_list).argmin()]\n",
    "best_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZmGv7hwXIei"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "best_parameter = grid[np.array(KL_list).argmin()]\n",
    "np.random.seed(10)\n",
    "\n",
    "print(best_parameter)\n",
    "X_transformed = TSNE(n_components=2, \n",
    "                  learning_rate=best_parameter['learning_rate'], \n",
    "                  init='random', \n",
    "                  perplexity=best_parameter['perplexity'],\n",
    "                  method = 'exact',\n",
    "                  early_exaggeration =best_parameter['early_exaggeration']).fit_transform(df_embedding.iloc[:,:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPrtmLqKXTBy"
   },
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(X_transformed, columns = ['d1', 'd2'])\n",
    "df_tsne['topic_label'] = df_embedding['topics'].values\n",
    "plt.figure(figsize = (6, 6))\n",
    "\n",
    "color = ['#EC7063', '#2980B9', '#27AE60', '#F1C40F']\n",
    "topic_list = ['Business & Finance', 'Politics & Government','Science & Mathematics', 'Computers & Internet']\n",
    "\n",
    "for i, j in zip(topic_list, color): \n",
    "  plt.scatter(df_tsne.loc[(df_tsne['topic_label'] == i),'d1'], \n",
    "              df_tsne.loc[(df_tsne['topic_label'] == i),'d2'],   \n",
    "              c = j, label = i, s = 5)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('dimension 1')\n",
    "plt.ylabel('dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKddHzA6CZuJ"
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl5tbjZDxfAi"
   },
   "source": [
    "### initialize sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIMRinMICiOS"
   },
   "outputs": [],
   "source": [
    "financial_tokenizer, financial_model = get_sentiment_model('ahmedrachid/FinancialBERT-Sentiment-Analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GNp6RJsxjKv"
   },
   "source": [
    "### extract logit from the sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQrYiLHj5KOq"
   },
   "outputs": [],
   "source": [
    "df_logit = logit_sent(news_data = news_data_sampled,\n",
    "                                financial_tokenizer = financial_tokenizer,\n",
    "                                financial_model = financial_model, \n",
    "                                summary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OxyugloOeJn"
   },
   "source": [
    "## Visualization: Original Text + Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMzZG7QZxrJT"
   },
   "source": [
    "### hyperparameter tuning of tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTVS4G6HHvTF"
   },
   "outputs": [],
   "source": [
    "parameter_dict = {'learning_rate': [50, 100, 150, 200],\n",
    "                  'perplexity': [10, 30, 50], \n",
    "                  'early_exaggeration': [20]}\n",
    "grid = ParameterGrid(parameter_dict)\n",
    "\n",
    "KL_list = []\n",
    "complete = 0\n",
    "for i in grid:\n",
    "  TSNE_model = TSNE(n_components=2, \n",
    "                    learning_rate=i['learning_rate'], \n",
    "                    init='random', \n",
    "                    perplexity=i['perplexity'],\n",
    "                    method = 'exact',\n",
    "                    early_exaggeration =i['early_exaggeration']).fit(df_logit.iloc[:,:-1])\n",
    "  KL_list.append(TSNE_model.kl_divergence_)\n",
    "  complete += 1\n",
    "  print(f'finish: {complete}/ {len(grid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaDuTILl-69x"
   },
   "source": [
    "### reduce data dimensionality by tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smpiY1SaICES"
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "best_parameter = grid[np.array(KL_list).argmin()]\n",
    "print(best_parameter)\n",
    "X_transformed_org_sent = TSNE(n_components=2,\n",
    "                              learning_rate=best_parameter['learning_rate'], \n",
    "                              init='random', \n",
    "                              perplexity=best_parameter['perplexity'],\n",
    "                              method = 'exact',\n",
    "                              early_exaggeration =best_parameter['early_exaggeration']).fit_transform(df_logit.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_TcXEe4_G8k"
   },
   "source": [
    "### visualize the text representation of sampled data (label: sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZNgldksIMFJ"
   },
   "outputs": [],
   "source": [
    "logit_visualize(df_logit, X_transformed_org_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QpHGVgnOlK5"
   },
   "source": [
    "## Viusalization: Summary + Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN2-jEUgcz5K"
   },
   "source": [
    "### extract logit of BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmfBX_vlOpQa"
   },
   "outputs": [],
   "source": [
    "df_logit_summary = logit_sent(news_data = news_data_sampled,\n",
    "                              financial_tokenizer = financial_tokenizer,\n",
    "                              financial_model = financial_model,\n",
    "                              summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meE4xs-kbIHM"
   },
   "source": [
    "### hyperpamaeter tuning of tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXEmdXjJbS7A"
   },
   "outputs": [],
   "source": [
    "parameter_dict = {'learning_rate': [50, 100, 150, 200],\n",
    "                  'perplexity': [10, 30, 50], \n",
    "                  'early_exaggeration': [20]}\n",
    "grid = ParameterGrid(parameter_dict)\n",
    "\n",
    "KL_list = []\n",
    "complete = 0\n",
    "for i in grid:\n",
    "  TSNE_model = TSNE(n_components=2, \n",
    "                    learning_rate=i['learning_rate'], \n",
    "                    init='random', \n",
    "                    perplexity=i['perplexity'],\n",
    "                    method = 'exact',\n",
    "                    early_exaggeration =i['early_exaggeration']).fit(df_logit_summary.iloc[:,:-1])\n",
    "  KL_list.append(TSNE_model.kl_divergence_)\n",
    "  complete += 1\n",
    "  print(f'finish: {complete}/ {len(grid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t70uS1Y5bClp"
   },
   "source": [
    "### reduce dimensionality by tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVz4TencOyYw"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "best_parameter = grid[np.array(KL_list).argmin()]\n",
    "np.random.seed(10)\n",
    "\n",
    "print(best_parameter)\n",
    "X_transformed_sum = TSNE(n_components=2,\n",
    "                         learning_rate=best_parameter['learning_rate'], \n",
    "                         init='random', \n",
    "                         perplexity=best_parameter['perplexity'],\n",
    "                         method = 'exact',\n",
    "                         early_exaggeration =best_parameter['early_exaggeration']).fit_transform(df_logit_summary.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9Y5UYioEV5U"
   },
   "outputs": [],
   "source": [
    "logit_visualize(df_logit_summary, X_transformed_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KXEW4c1J4Sy"
   },
   "source": [
    "## Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1uIjOvhKUyw"
   },
   "outputs": [],
   "source": [
    "topic_tokenizer, topic_model = get_topic_model('jonaskoenig/topic_classification_04')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0-03p0MPQCX"
   },
   "source": [
    "## Visualization: Original Text + Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q92K1nRsIO2G"
   },
   "outputs": [],
   "source": [
    "df_logit_original_topic = logit_topic(news_data = news_data_sampled, topic_tokenizer = topic_tokenizer, topic_model = topic_model, summary =  False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y2ScM0PebEr"
   },
   "source": [
    "### hyperparameter tuning of tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgmMWMlKeeMI"
   },
   "outputs": [],
   "source": [
    "parameter_dict = {'learning_rate': [50, 100, 150, 200],\n",
    "                  'perplexity': [10, 30, 50], \n",
    "                  'early_exaggeration': [20]}\n",
    "grid = ParameterGrid(parameter_dict)\n",
    "\n",
    "KL_list = []\n",
    "complete = 0\n",
    "for i in grid:\n",
    "  TSNE_model = TSNE(n_components=2, \n",
    "                    learning_rate=i['learning_rate'], \n",
    "                    init='random', \n",
    "                    perplexity=i['perplexity'],\n",
    "                    method = 'exact',\n",
    "                    early_exaggeration =i['early_exaggeration']).fit(df_logit_original_topic.iloc[:,:-1])\n",
    "  KL_list.append(TSNE_model.kl_divergence_)\n",
    "  complete += 1\n",
    "  print(f'finish: {complete}/ {len(grid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JijIRvQGj3FB"
   },
   "source": [
    "### reduce dimensionality by tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVQM4q29LCzk"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "best_parameter = grid[np.array(KL_list).argmin()]\n",
    "np.random.seed(10)\n",
    "\n",
    "print(best_parameter)\n",
    "X_transformed_topic = TSNE(n_components=2,\n",
    "                           learning_rate=best_parameter['learning_rate'], \n",
    "                           init='random',\n",
    "                           perplexity=best_parameter['perplexity'],\n",
    "                           method = 'exact',\n",
    "                           early_exaggeration =best_parameter['early_exaggeration']).fit_transform(df_logit_original_topic.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X40P41_ZLVHH"
   },
   "outputs": [],
   "source": [
    "logit_topic_visualize(news_data_sampled, X_transformed_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcbVlP_aPuG3"
   },
   "source": [
    "## Visualization: Summary + Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n09qUXi1PyQn"
   },
   "outputs": [],
   "source": [
    "df_logit_summary_topic = logit_topic(news_data = news_data_sampled, topic_tokenizer = topic_tokenizer, topic_model = topic_model, summary =  True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "by5ojMLZkpET"
   },
   "source": [
    "### hyperparameter tuning of tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icOLykGlksxA"
   },
   "outputs": [],
   "source": [
    "parameter_dict = {'learning_rate': [50, 100, 150, 200],\n",
    "                  'perplexity': [10, 30, 50], \n",
    "                  'early_exaggeration': [20]}\n",
    "grid = ParameterGrid(parameter_dict)\n",
    "\n",
    "KL_list = []\n",
    "complete = 0\n",
    "for i in grid:\n",
    "  TSNE_model = TSNE(n_components=2, \n",
    "                    learning_rate=i['learning_rate'], \n",
    "                    init='random', \n",
    "                    perplexity=i['perplexity'],\n",
    "                    method = 'exact',\n",
    "                    early_exaggeration =i['early_exaggeration']).fit(df_logit_summary_topic.iloc[:,:-1])\n",
    "  KL_list.append(TSNE_model.kl_divergence_)\n",
    "  complete += 1\n",
    "  print(f'finish: {complete}/ {len(grid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0bMQ100lNik"
   },
   "source": [
    "### reduce dimensionality by using tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61c3-JO7PyHk"
   },
   "outputs": [],
   "source": [
    "best_parameter = grid[np.array(KL_list).argmin()]\n",
    "np.random.seed(10)\n",
    "\n",
    "print(best_parameter)\n",
    "X_transformed_sum_topic = TSNE(n_components=2,\n",
    "                               learning_rate=best_parameter['learning_rate'],\n",
    "                               init='random',\n",
    "                               perplexity=best_parameter['perplexity'],\n",
    "                               method = 'exact',\n",
    "                               early_exaggeration =best_parameter['early_exaggeration']).fit_transform(df_logit_summary_topic.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOR6_qm3PyAH"
   },
   "outputs": [],
   "source": [
    "logit_topic_visualize(news_data_sampled, X_transformed_sum_topic)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPw8ItZ7Ao0xMBCpRM8E9O+",
   "collapsed_sections": [
    "Ro9hpuKNjzqj",
    "Y5bOfaE4m5bk"
   ],
   "machine_shape": "hm",
   "mount_file_id": "1QWfNtyxh_86ADZHGw4DzAd7Q-iE72quM",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
